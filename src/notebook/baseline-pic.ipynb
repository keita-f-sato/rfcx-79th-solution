{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW, lr_scheduler\n",
    "from torch.distributions import Uniform\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.models import resnet34, resnet50\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from libs import transform as tr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tp = pd.read_csv('../../data/train_tp.csv')\n",
    "submission = pd.read_csv('../../data/sample_submission.csv')\n",
    "\n",
    "pred_target = list(submission.columns)[1:]\n",
    "\n",
    "\n",
    "SR = 48000\n",
    "LEN_10SEC = SR * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_melspec(X: np.ndarray):\n",
    "    eps = 1e-6\n",
    "    mean = X.mean()\n",
    "    X = X - mean\n",
    "    std = X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    norm_min, norm_max = Xstd.min(), Xstd.max()\n",
    "    if (norm_max - norm_min) > eps:\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RfcxDataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 df:pd.DataFrame,\n",
    "                 data_path:str,\n",
    "                 melspectrogram_parameters:dict,\n",
    "                 pcen_parameters:dict\n",
    "    ):\n",
    "        self.df =  df\n",
    "        self.path = data_path\n",
    "        self.img_size = 256\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "        self.pcen_parameters = pcen_parameters\n",
    "        \n",
    "        \n",
    "        self.transform = tr.Compose([\n",
    "          tr.OneOf([\n",
    "            tr.GaussianNoiseSNR(min_snr=10),\n",
    "            tr.PinkNoiseSNR(min_snr=10)\n",
    "          ]),\n",
    "          #tr.PitchShift(max_steps=2, sr=SR),\n",
    "          #tr.TimeStretch(),\n",
    "          #tr.TimeShift(sr=SR),\n",
    "          tr.VolumeControl(mode=\"sine\")\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.iloc[idx, :]\n",
    "        recording_id = sample['recording_id']\n",
    "        species_id = sample['species_id']\n",
    "        t_min = int(round(sample['t_min']))\n",
    "        t_max = int(round(sample['t_max']))\n",
    "        \n",
    "        annotated_duration = t_max - t_min\n",
    "        \n",
    "        if annotated_duration > 15:\n",
    "            limit_sec = t_max - 10\n",
    "            start_sec = random.randint(t_min, limit_sec)\n",
    "            end_sec = start_sec + 10\n",
    "            \n",
    "            start = (start_sec - 7) * SR\n",
    "            end = (end_sec + 8) * SR\n",
    "        else:\n",
    "            res_time = 10 - annotated_duration\n",
    "            front_limit = res_time if res_time < t_min else t_min\n",
    "            \n",
    "            front_time = random.randint(0, front_limit)\n",
    "            back_time = res_time - front_time\n",
    "            \n",
    "            start = 0\n",
    "            end = (t_max + back_time + 10) * SR\n",
    "\n",
    "\n",
    "                    \n",
    "        record_path = self.path + recording_id + '.flac'\n",
    "        y, sr = librosa.load(record_path, sr=None)\n",
    "\n",
    "        tarnsform_flag = random.choice([True, True, False])\n",
    "        \n",
    "        if tarnsform_flag:\n",
    "            y = self.transform(y[start:end])\n",
    "        \n",
    "        \n",
    "        melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspectrogram_parameters)\n",
    "        pcen = librosa.pcen(melspec, sr=SR, **self.pcen_parameters)\n",
    "        clean_mel = librosa.power_to_db(melspec ** 1.5)\n",
    "        melspec = librosa.power_to_db(melspec)\n",
    "        \n",
    "        norm_melspec = normalize_melspec(melspec)\n",
    "        norm_pcen = normalize_melspec(pcen)\n",
    "        norm_clean_mel = normalize_melspec(clean_mel)\n",
    "        image = np.stack([norm_melspec, norm_pcen, norm_clean_mel], axis=-1)\n",
    "        \n",
    "        height, width, _ = image.shape\n",
    "        limit = width - 937\n",
    "        \n",
    "        start = random.randint(0, limit)\n",
    "        \n",
    "        image = cv2.resize(image[:, start:start+430, :], (self.img_size*2, self.img_size))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "        \n",
    "        target = torch.zeros([24], dtype=torch.float32)\n",
    "        target[species_id] = 1\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RfcxTestDataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 df:pd.DataFrame,\n",
    "                 data_path:str,\n",
    "                 melspectrogram_parameters:dict,\n",
    "                 pcen_parameters:dict,\n",
    "                 val=False\n",
    "    ):\n",
    "        self.df =  df\n",
    "        self.val = val\n",
    "        self.path = data_path\n",
    "        self.img_size = 256\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "        self.pcen_parameters = pcen_parameters\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.iloc[idx, :]\n",
    "        recording_id = sample['recording_id']\n",
    "        record_path = self.path + recording_id + '.flac'\n",
    "        \n",
    "        y, sr = librosa.load(record_path, sr=None)\n",
    "        \n",
    "        species_id = sample['species_id']\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspectrogram_parameters)\n",
    "        pcen = librosa.pcen(melspec, sr=SR, **self.pcen_parameters)\n",
    "        clean_mel = librosa.power_to_db(melspec ** 1.5)\n",
    "\n",
    "        norm_melspec = normalize_melspec(melspec)\n",
    "        norm_pcen = normalize_melspec(pcen)\n",
    "        norm_clean_mel = normalize_melspec(clean_mel)\n",
    "        image = np.stack([norm_melspec, norm_pcen, norm_clean_mel], axis=-1)\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(6):\n",
    "            end = start + 937\n",
    "            height, width, _ = image.shape\n",
    "            tmp_image = cv2.resize(image[:, start:end, :], (self.img_size*2, self.img_size))\n",
    "            tmp_image = np.moveaxis(tmp_image, 2, 0)\n",
    "            tmp_image = (tmp_image / 255.0).astype(np.float32)\n",
    "            images.append(tmp_image)\n",
    "            \n",
    "            start = end\n",
    "            \n",
    "        if self.val == True:\n",
    "            target = torch.zeros([24], dtype=torch.float32)\n",
    "            target[species_id] = 1\n",
    "            return images, target\n",
    "        else:\n",
    "            return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_type, model_name, output_size, spectrogram_params, logmel_extractor_params, spec_augmenter_params):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if model_type == 'res':\n",
    "            if model_name == 'resnet50':\n",
    "                base_model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n",
    "            else:\n",
    "                base_model = resnet34(pretrained=True)\n",
    "            \n",
    "            layers = list(base_model.children())[:-1]\n",
    "            self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "            in_features = base_model.fc.in_features\n",
    "\n",
    "            self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "            \n",
    "            \"\"\"self.fc1 = nn.Sequential(\n",
    "                    nn.Linear(in_features, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=0.2),\n",
    "                    nn.Linear(1024, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=0.2),\n",
    "                    nn.Linear(1024, num_birds)\n",
    "                )\"\"\"\n",
    "\n",
    "            self.classification = nn.Linear(in_features, output_size)\n",
    "            \n",
    "        elif model_type == 'efficientnet':\n",
    "            self.features = EfficientNet.from_pretrained(model_name)\n",
    "            model_code = model_name.split('-')[1]\n",
    "            \n",
    "            num_input = {\n",
    "                'b4': 1792,\n",
    "                'b2': 1408,\n",
    "                'b7': 2560\n",
    "            }\n",
    "            \n",
    "            self.features_out = num_input[model_code]\n",
    "            \n",
    "            self.classification = nn.Sequential(nn.Linear(num_input[model_code], output_size))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.model_type == 'res':\n",
    "            x = self.encoder(input)\n",
    "\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = F.relu_(self.fc1(x))\n",
    "            \n",
    "            x = self.classification(x)\n",
    "            \n",
    "        elif self.model_type == 'efficientnet':\n",
    "            x = self.features.extract_features(input)\n",
    "            x = F.avg_pool2d(x, x.size()[2:]).reshape(-1, self.features_out)\n",
    "            x = self.classification(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(fold, model, optim, criterion, file_path=\"../../model/\"):\n",
    "    if not TEST_NAME in os.listdir(file_path):\n",
    "        os.mkdir(file_path+TEST_NAME)\n",
    "    \n",
    "    \n",
    "    output_path = file_path + TEST_NAME + '/' + f\"{TEST_NAME}_{fold}.model\"\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.cpu().state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'criterion': criterion\n",
    "        },\n",
    "        output_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRAP. Instance-level average\n",
    "# Assume float preds [BxC], labels [BxC] of 0 or 1\n",
    "def LRAP(preds, labels):\n",
    "    # Ranks of the predictions\n",
    "    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n",
    "    # i, j corresponds to rank of prediction in row i\n",
    "    class_ranks = torch.zeros_like(ranked_classes)\n",
    "    for i in range(ranked_classes.size(0)):\n",
    "        for j in range(ranked_classes.size(1)):\n",
    "            class_ranks[i, ranked_classes[i][j]] = j + 1\n",
    "    # Mask out to only use the ranks of relevant GT labels\n",
    "    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n",
    "    # All the GT ranks are in front now\n",
    "    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n",
    "    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n",
    "    score_matrix = pos_matrix / sorted_ground_truth_ranks\n",
    "    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n",
    "    scores = score_matrix * score_mask_matrix\n",
    "    score = (scores.sum(-1) / labels.sum(-1)).mean()\n",
    "    return score.item()\n",
    "\n",
    "# label-level average\n",
    "# Assume float preds [BxC], labels [BxC] of 0 or 1\n",
    "def LWLRAP(preds, labels):\n",
    "    # Ranks of the predictions\n",
    "    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n",
    "    # i, j corresponds to rank of prediction in row i\n",
    "    class_ranks = torch.zeros_like(ranked_classes)\n",
    "    for i in range(ranked_classes.size(0)):\n",
    "        for j in range(ranked_classes.size(1)):\n",
    "            class_ranks[i, ranked_classes[i][j]] = j + 1\n",
    "    # Mask out to only use the ranks of relevant GT labels\n",
    "    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n",
    "    # All the GT ranks are in front now\n",
    "    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n",
    "    # Number of GT labels per instance\n",
    "    num_labels = labels.sum(-1)\n",
    "    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n",
    "    score_matrix = pos_matrix / sorted_ground_truth_ranks\n",
    "    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n",
    "    scores = score_matrix * score_mask_matrix\n",
    "    score = scores.sum() / labels.sum()\n",
    "    return score.item()\n",
    "\n",
    "# Sample usage\n",
    "# y_true = torch.tensor(np.array([[1, 1, 0], [1, 0, 1], [0, 0, 1]]))\n",
    "# y_score = torch.tensor(np.random.randn(3, 3))\n",
    "# print(LRAP(y_score, y_true), LWLRAP(y_score, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NAME = 'baseline-pic-res'\n",
    "\n",
    "n_splits = 5\n",
    "random_state = 1\n",
    "epochs = 35\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "spectrogram_params = {\n",
    "    'n_fft': 1024,\n",
    "    'hop_length': 320,\n",
    "    'win_length': 1024,\n",
    "    'window': 'hann',\n",
    "    'center': True,\n",
    "    'pad_mode': 'reflect',\n",
    "    'freeze_parameters': True\n",
    "}\n",
    "\n",
    "logmel_extractor_params = {\n",
    "    'sr': SR,\n",
    "    'n_fft': 1024,\n",
    "    'n_mels': 64,\n",
    "    'fmin': 50,\n",
    "    'fmax': 20000,\n",
    "    'ref' : 1.0,\n",
    "    'amin': 1e-10,\n",
    "    'top_db': None,\n",
    "    'freeze_parameters': True\n",
    "}\n",
    "\n",
    "spec_augmenter_params = {\n",
    "    'time_drop_width':  64,\n",
    "    'time_stripes_num': 2,\n",
    "    'freq_drop_width':  8,\n",
    "    'freq_stripes_num': 2\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'model_type': 'res', # res or efficientnet\n",
    "    'model_name': 'resnet50',\n",
    "    'output_size': 24,\n",
    "    'spectrogram_params': spectrogram_params,\n",
    "    'logmel_extractor_params': logmel_extractor_params,\n",
    "    'spec_augmenter_params': spec_augmenter_params,\n",
    "}\n",
    "\n",
    "optim_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 5e-5,\n",
    "    'betas': (0.9, 0.999)\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "    'mode': 'max',\n",
    "    'patience': 1,\n",
    "    'factor': 0.4,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "data_params = {\n",
    "    'data_path': '/home/yuigahama/kaggle/rfcx/data/train/',\n",
    "    'melspectrogram_parameters': {\n",
    "        'n_mels': 128,\n",
    "        'fmin': 20,\n",
    "        'fmax': 16000,\n",
    "    },\n",
    "    'pcen_parameters': {\n",
    "        'gain': 0.98,\n",
    "        'bias': 2,\n",
    "        'power': 0.5,\n",
    "        'time_constant': 0.4,\n",
    "        'eps': 0.000001,\n",
    "    }\n",
    "}\n",
    "\n",
    "test_data_params = {\n",
    "    'data_path': '/home/yuigahama/kaggle/rfcx/data/test_tensor_mel_48000/',\n",
    "    'melspectrogram_parameters': {\n",
    "        'n_mels': 128,\n",
    "        'fmin': 20,\n",
    "        'fmax': 16000,\n",
    "    },\n",
    "    'pcen_parameters': {\n",
    "        'gain': 0.98,\n",
    "        'bias': 2,\n",
    "        'power': 0.5,\n",
    "        'time_constant': 0.4,\n",
    "        'eps': 0.000001,\n",
    "    }\n",
    "}\n",
    "\n",
    "dataloder_params = {\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 15,\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "test_dataloder_params = {\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 15,\n",
    "    'pin_memory': True,\n",
    "    'shuffle':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- fold 0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yuigahama/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1| T | loss: 0.235 | score: 0.183 | V | loss: 0.227 | score: 0.204 | time: 0:01:17\n",
      "epoch   2| T | loss: 0.172 | score: 0.213 | V | loss: 0.172 | score: 0.23 | time: 0:01:19\n",
      "epoch   3| T | loss: 0.161 | score: 0.28 | V | loss: 0.161 | score: 0.285 | time: 0:01:14\n",
      "epoch   4| T | loss: 0.154 | score: 0.337 | V | loss: 0.165 | score: 0.283 | time: 0:01:15\n",
      "epoch   5| T | loss: 0.149 | score: 0.368 | V | loss: 0.161 | score: 0.29 | time: 0:01:16\n",
      "epoch   6| T | loss: 0.141 | score: 0.416 | V | loss: 0.152 | score: 0.388 | time: 0:01:15\n",
      "epoch   7| T | loss: 0.136 | score: 0.463 | V | loss: 0.144 | score: 0.414 | time: 0:01:15\n",
      "epoch   8| T | loss: 0.132 | score: 0.477 | V | loss: 0.14 | score: 0.438 | time: 0:01:17\n",
      "epoch   9| T | loss: 0.127 | score: 0.514 | V | loss: 0.164 | score: 0.365 | time: 0:01:16\n",
      "epoch  10| T | loss: 0.125 | score: 0.527 | V | loss: 0.15 | score: 0.379 | time: 0:01:14\n",
      "epoch  11| T | loss: 0.119 | score: 0.554 | V | loss: 0.149 | score: 0.41 | time: 0:01:15\n",
      "epoch  12| T | loss: 0.112 | score: 0.58 | V | loss: 0.139 | score: 0.441 | time: 0:01:14\n",
      "epoch  13| T | loss: 0.112 | score: 0.578 | V | loss: 0.184 | score: 0.333 | time: 0:01:15\n",
      "epoch  14| T | loss: 0.103 | score: 0.634 | V | loss: 0.161 | score: 0.478 | time: 0:01:15\n",
      "epoch  15| T | loss: 0.107 | score: 0.597 | V | loss: 0.149 | score: 0.445 | time: 0:01:16\n",
      "epoch  16| T | loss: 0.0999 | score: 0.649 | V | loss: 0.15 | score: 0.473 | time: 0:01:15\n",
      "epoch  17| T | loss: 0.0964 | score: 0.675 | V | loss: 0.144 | score: 0.438 | time: 0:01:16\n",
      "epoch  18| T | loss: 0.0913 | score: 0.694 | V | loss: 0.203 | score: 0.439 | time: 0:01:15\n",
      "epoch  19| T | loss: 0.0909 | score: 0.688 | V | loss: 0.165 | score: 0.443 | time: 0:01:17\n",
      "epoch  20| T | loss: 0.0871 | score: 0.71 | V | loss: 0.127 | score: 0.523 | time: 0:01:14\n",
      "epoch  21| T | loss: 0.0812 | score: 0.739 | V | loss: 0.143 | score: 0.525 | time: 0:01:19\n",
      "epoch  22| T | loss: 0.0791 | score: 0.733 | V | loss: 0.151 | score: 0.519 | time: 0:01:16\n",
      "epoch  23| T | loss: 0.0726 | score: 0.773 | V | loss: 0.162 | score: 0.487 | time: 0:01:12\n",
      "epoch  24| T | loss: 0.0733 | score: 0.763 | V | loss: 0.158 | score: 0.469 | time: 0:01:14\n",
      "epoch  25| T | loss: 0.069 | score: 0.783 | V | loss: 0.146 | score: 0.547 | time: 0:01:16\n",
      "epoch  26| T | loss: 0.0665 | score: 0.789 | V | loss: 0.132 | score: 0.558 | time: 0:01:15\n",
      "epoch  27| T | loss: 0.0629 | score: 0.798 | V | loss: 0.151 | score: 0.514 | time: 0:01:13\n",
      "epoch  28| T | loss: 0.0607 | score: 0.814 | V | loss: 0.133 | score: 0.559 | time: 0:01:13\n",
      "epoch  29| T | loss: 0.0582 | score: 0.829 | V | loss: 0.14 | score: 0.566 | time: 0:01:18\n",
      "epoch  30| T | loss: 0.0613 | score: 0.818 | V | loss: 0.146 | score: 0.541 | time: 0:01:16\n",
      "epoch  31| T | loss: 0.056 | score: 0.837 | V | loss: 0.146 | score: 0.534 | time: 0:01:17\n",
      "epoch  32| T | loss: 0.0565 | score: 0.833 | V | loss: 0.15 | score: 0.532 | time: 0:01:15\n",
      "epoch  33| T | loss: 0.0524 | score: 0.854 | V | loss: 0.146 | score: 0.552 | time: 0:01:13\n",
      "epoch  34| T | loss: 0.0517 | score: 0.85 | V | loss: 0.148 | score: 0.545 | time: 0:01:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yuigahama/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\", line 146, in load\n    with sf.SoundFile(path) as sf_desc:\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/soundfile.py\", line 629, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/soundfile.py\", line 1183, in _open\n    _error_check(_snd.sf_error(file_ptr),\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/soundfile.py\", line 1357, in _error_check\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\nRuntimeError: Error opening '/home/yuigahama/kaggle/rfcx/data/test_tensor_mel_48000/000316da7.flac': System error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-d55b8c519219>\", line 24, in __getitem__\n    y, sr = librosa.load(record_path, sr=None)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\", line 163, in load\n    y, sr_native = __audioread_load(path, offset, duration, dtype)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\", line 187, in __audioread_load\n    with audioread.audio_open(path) as input_file:\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/audioread/__init__.py\", line 111, in audio_open\n    return BackendClass(path)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/audioread/rawread.py\", line 62, in __init__\n    self._fh = open(filename, 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: '/home/yuigahama/kaggle/rfcx/data/test_tensor_mel_48000/000316da7.flac'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-505a98356cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mtmp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\", line 146, in load\n    with sf.SoundFile(path) as sf_desc:\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/soundfile.py\", line 629, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/soundfile.py\", line 1183, in _open\n    _error_check(_snd.sf_error(file_ptr),\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/soundfile.py\", line 1357, in _error_check\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\nRuntimeError: Error opening '/home/yuigahama/kaggle/rfcx/data/test_tensor_mel_48000/000316da7.flac': System error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-d55b8c519219>\", line 24, in __getitem__\n    y, sr = librosa.load(record_path, sr=None)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\", line 163, in load\n    y, sr_native = __audioread_load(path, offset, duration, dtype)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\", line 187, in __audioread_load\n    with audioread.audio_open(path) as input_file:\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/audioread/__init__.py\", line 111, in audio_open\n    return BackendClass(path)\n  File \"/home/yuigahama/anaconda3/lib/python3.8/site-packages/audioread/rawread.py\", line 62, in __init__\n    self._fh = open(filename, 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: '/home/yuigahama/kaggle/rfcx/data/test_tensor_mel_48000/000316da7.flac'\n"
     ]
    }
   ],
   "source": [
    "tta = np.zeros((len(submission), 24))\n",
    "cv_score = 0\n",
    "\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(train_tp, train_tp.species_id)):\n",
    "    print(f'---------- fold {fold_id} ----------')\n",
    "    \n",
    "    model = BaseModel(**model_params).to(device)\n",
    "    optim = Adam(model.parameters(), **optim_params)\n",
    "    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optim, **scheduler_params)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=epochs)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_dataset = RfcxDataSet(train_tp.iloc[train_index], **data_params)\n",
    "    val_dataset = RfcxTestDataSet(train_tp.iloc[val_index], val=True, **data_params)\n",
    "    test_dataset = RfcxTestDataSet(submission, **test_data_params)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, **dataloder_params)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=False, **dataloder_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **test_dataloder_params)\n",
    "    \n",
    "    es = 15\n",
    "    for epoch in range(1, epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        bast_score = 0\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            image = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "            \n",
    "            pred = model(image)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss = criterion(pred, label)\n",
    "            score = LWLRAP(pred.cpu(), label.cpu())\n",
    "            \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_score += score\n",
    "            \n",
    "        train_loss  /= len(train_dataloader)\n",
    "        train_score /= len(train_dataloader)\n",
    "        \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_data in val_dataloader:\n",
    "                num = len(val_data[0])\n",
    "                label = val_data[1]\n",
    "                tmp = torch.zeros((val_data[0][0].size(0), 24))\n",
    "                \n",
    "                for img in val_data[0]:\n",
    "                    pred = model(img.to(device))\n",
    "                    tmp += pred.cpu() / num\n",
    "                \n",
    "                val_loss += criterion(tmp, label).item()\n",
    "                val_score += LWLRAP(tmp, label)\n",
    "\n",
    "                \n",
    "        val_loss  /= len(val_dataloader)\n",
    "        val_score /= len(val_dataloader)\n",
    "        \n",
    "        duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
    "        print(f'epoch {epoch:3}| T | loss: {train_loss:.3} | score: {train_score:.3} | V | loss: {val_loss:.3} | score: {val_score:.3} | time: {duration}')\n",
    "\n",
    "        if bast_score < val_score:\n",
    "            bast_score = val_score\n",
    "            bast_path = save(fold_id, model, optim, criterion)\n",
    "        else:\n",
    "            es -= 1\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        if es == 0:\n",
    "            break\n",
    "        \n",
    "    cv_score += bast_score / 5\n",
    "        \n",
    "    oof = np.zeros((len(test_dataset), 24))\n",
    "    model = BaseModel(**model_params).to(device)\n",
    "    \n",
    "    bast_model_parms = torch.load(bast_path)\n",
    "    model.load_state_dict(bast_model_parms['model_state_dict'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start = 0\n",
    "        for i, test_data in enumerate(test_dataloader):\n",
    "            target_size = test_data[0].size(0)\n",
    "            tmp_pred = np.zeros((target_size, 24))\n",
    "            num = len(test_data)\n",
    "            end = start + target_size\n",
    "            for img in test_data:\n",
    "                img = img.to(device)\n",
    "                pred = model(img)\n",
    "                \n",
    "                tmp_pred += torch.sigmoid(pred.cpu().detach()).numpy() / num\n",
    "                       \n",
    "            oof[start:end, :] = tmp_pred\n",
    "            start = end\n",
    "            \n",
    "    tta += (oof / n_splits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sub = pd.DataFrame(tta, columns=pred_target)\n",
    "pred_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([submission['recording_id'], pred_sub], axis=1).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
