{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuigahama/anaconda3/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW, lr_scheduler\n",
    "from torch.distributions import Uniform\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.models import resnet34, resnet50\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from libs import transform as tr\n",
    "from libs import spectrogram as spec\n",
    "from libs import criterion as cr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "set_seed(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tp = pd.read_csv('../../data/train_tp.csv')\n",
    "train_fp = pd.read_csv('../../data/train_fp.csv')\n",
    "submission = pd.read_csv('../../data/sample_submission.csv')\n",
    "\n",
    "pred_target = list(submission.columns)[1:]\n",
    "\n",
    "SR = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_melspec(X: np.ndarray):\n",
    "    eps = 1e-6\n",
    "    mean = X.mean()\n",
    "    X = X - mean\n",
    "    std = X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    norm_min, norm_max = Xstd.min(), Xstd.max()\n",
    "    if (norm_max - norm_min) > eps:\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "def save(fold, model, optim, criterion, file_path=\"../../model/\"):\n",
    "    if not TEST_NAME in os.listdir(file_path):\n",
    "        os.mkdir(file_path+TEST_NAME)\n",
    "    \n",
    "    \n",
    "    output_path = file_path + TEST_NAME + '/' + f\"{TEST_NAME}_{fold}.model\"\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.cpu().state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'criterion': criterion\n",
    "        },\n",
    "        output_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RfcxDataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 tp:pd.DataFrame,\n",
    "                 train: bool,\n",
    "                 data_path:str,\n",
    "                 pcen_parameters:dict,\n",
    "                 pre_calc=True,\n",
    "                 n_mels=128\n",
    "    ):\n",
    "        self.tp = tp\n",
    "        self.path = data_path\n",
    "        self.img_size = 256\n",
    "        self.train = train\n",
    "        self.n_mels = n_mels\n",
    "        self.pre_calc = pre_calc\n",
    "        \n",
    "        self.transform = tr.Compose([\n",
    "            tr.OneOf([\n",
    "                tr.GaussianNoiseSNR(min_snr=10),\n",
    "                tr.PinkNoiseSNR(min_snr=10)\n",
    "            ]),\n",
    "            tr.PitchShift(max_steps=2, sr=SR),\n",
    "            #tr.TimeStretch(),\n",
    "            #tr.TimeShift(sr=sr),\n",
    "            tr.VolumeControl(mode=\"sine\")\n",
    "        ])\n",
    "        \n",
    "        self.pcen_parameters = pcen_parameters\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tp)\n",
    "    \n",
    "    def load(self, record_path):\n",
    "        y, orig_sr = sf.read(record_path)\n",
    "        \n",
    "        if orig_sr != SR:\n",
    "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=SR, res_type=\"kaiser_best\")\n",
    "        return y\n",
    "    \n",
    "    def get_random_duration(self, duration=10):\n",
    "        start_sec = random.randint(0, 60-duration)\n",
    "        end_sec = start_sec + 10\n",
    "            \n",
    "        return start_sec, end_sec\n",
    "    \n",
    "    def get_duration(self, t_min, t_max, duration=10):\n",
    "        annotated_duration = t_max - t_min\n",
    "        \n",
    "        if annotated_duration > duration:\n",
    "            limit_sec = t_max - duration\n",
    "            start_sec = random.randint(t_min, limit_sec)\n",
    "            end_sec = start_sec + duration\n",
    "\n",
    "        else:\n",
    "            res_time = duration - annotated_duration\n",
    "            front_limit = res_time if res_time < t_min else t_min\n",
    "            \n",
    "            front_time = random.randint(0, front_limit)\n",
    "            \n",
    "            back_limit = 60 - t_max\n",
    "            \n",
    "            tmp_time = res_time - front_time\n",
    "            back_time = tmp_time if tmp_time < back_limit else back_limit\n",
    "            \n",
    "            if not tmp_time < back_limit:\n",
    "                front_time += tmp_time - back_limit\n",
    "            \n",
    "            start_sec = t_min - front_time\n",
    "            end_sec = t_max + back_time\n",
    "            \n",
    "        return start_sec, end_sec\n",
    "    \n",
    "    def create_mel(self, y):\n",
    "        y = self.transform(y)\n",
    "        \n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            y,\n",
    "            sr=SR,\n",
    "            fmin=0,\n",
    "            fmax=15000,\n",
    "            n_mels=128\n",
    "        )\n",
    "\n",
    "        pcen = librosa.pcen(melspec, sr=SR, **self.pcen_parameters)\n",
    "        clean_mel = librosa.power_to_db(melspec ** 1.5)\n",
    "        melspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        norm_melspec = normalize_melspec(melspec)\n",
    "        norm_pcen = normalize_melspec(pcen)\n",
    "        norm_clean_mel = normalize_melspec(clean_mel)\n",
    "\n",
    "        image = np.stack([norm_melspec, norm_pcen, norm_clean_mel], axis=-1)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.tp.iloc[idx, :]\n",
    "        recording_id = sample['recording_id']\n",
    "        t_min = int(round(sample['t_min']))\n",
    "        t_max = int(round(sample['t_max']))\n",
    "        \n",
    "        start_sec, end_sec = self.get_duration(t_min, t_max, 10)\n",
    "            \n",
    "        record_path = self.path + recording_id + '.flac'\n",
    "        y = self.load(record_path)\n",
    "        y =  y[start_sec*SR:end_sec*SR]\n",
    "        \n",
    "        if self.train:\n",
    "            y = self.transform(y)\n",
    "        \n",
    "        species_id = sample['species_id']\n",
    "        target = torch.zeros([24], dtype=torch.float32)\n",
    "        target[species_id] = 1\n",
    "        \n",
    "        return y, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n",
    "    (1, 3, 5, ...).\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n",
    "           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Mixup(object):\n",
    "    def __init__(self, mixup_alpha, random_seed=1234):\n",
    "        \"\"\"Mixup coefficient generator.\n",
    "        \"\"\"\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def get_lambda(self, batch_size):\n",
    "        \"\"\"Get mixup random coefficients.\n",
    "        Args:\n",
    "          batch_size: int\n",
    "        Returns:\n",
    "          mixup_lambdas: (batch_size,)\n",
    "        \"\"\"\n",
    "        mixup_lambdas = []\n",
    "        for n in range(0, batch_size, 2):\n",
    "            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "            mixup_lambdas.append(lam)\n",
    "            mixup_lambdas.append(1. - lam)\n",
    "\n",
    "        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1:, :].repeat(\n",
    "        1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetSED(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str,\n",
    "        pretrained=False,\n",
    "        num_classes=24,\n",
    "        spectrogram_params={},\n",
    "        logmel_extractor_params={},\n",
    "        spec_augmenter_params={},\n",
    "        pce_params={}\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spectrogram_extractor = Spectrogram(**spectrogram_params)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(**logmel_extractor_params)\n",
    "        \n",
    "        #Pcen converter\n",
    "        self.pcen_converter = spec.pcen(**pce_params)\n",
    "\n",
    "        # Spec augmenter\n",
    "        #self.spec_augmenter = SpecAugmentation(**spec_augmenter_params)\n",
    "        \n",
    "        self.interpolate_ratio = 30  # Downsampled ratio\n",
    "        self.mixup_alpha = 0.2\n",
    "        self.random_state = np.random.RandomState(123)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.base_model = EfficientNet.from_pretrained(base_model_name)\n",
    "        else:\n",
    "            self.base_model = EfficientNet.from_name(base_model_name)\n",
    "\n",
    "        in_features = self.base_model._fc.in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def mixup(self, x):\n",
    "        sizws = x.size()\n",
    "        #lam = torch.from_numpy(self.random_state.beta(self.mixup_alpha, self.mixup_alpha, (sizws[0], 1))).cuda()\n",
    "        lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "        index = list(range(x.size(0)))\n",
    "        random.shuffle(index)\n",
    "        #out = (x.view(sizws[0], -1) * lam + x[index].squeeze().view(sizws[0], -1) * (1-lam)).view(sizws[0], sizws[1], sizws[2], sizws[3],)\n",
    "        out = (x * lam + x[index].squeeze() * (1-lam))\n",
    "        return out.float(), {'lam': lam, 'index': index}\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "    def forward(self, input):        \n",
    "        x = self.spectrogram_extractor(input)\n",
    "        x = self.logmel_extractor(x)\n",
    "        \n",
    "        x_mels = self.logmel_extractor.power_to_db(x)\n",
    "        x_pcen = self.pcen_converter(x) \n",
    "        x_clear = self.logmel_extractor.power_to_db(x ** 1.5)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((x_mels,x_pcen,x_clear),1)\n",
    "        #x = torch.cat((x,x,x),1)\n",
    "        \n",
    "        frames_num = x.size(2)\n",
    "        \n",
    "        if self.training:\n",
    "            x, mix_info = self.mixup(x)\n",
    "            #x = self.spec_augmenter(x)\n",
    "        else:\n",
    "            mix_info = None\n",
    "                \n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.base_model.extract_features(x)\n",
    "\n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=3)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, self.interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict, mix_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logit, target, mixup_info=None):\n",
    "        target = target.float()\n",
    "        max_val = (-logit).clamp(min=0)\n",
    "        loss = logit - logit * target + max_val + \\\n",
    "            ((-max_val).exp() + (-logit - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        if len(loss.size()) == 2:\n",
    "            loss = loss.sum(dim=1)\n",
    "            \n",
    "        if mixup_info is None:\n",
    "            return loss.mean()\n",
    "        \n",
    "        target = target[mixup_info['index']].float()\n",
    "        logit = logit[mixup_info['index']]\n",
    "        \n",
    "        max_val = (-logit).clamp(min=0)\n",
    "        loss2 = logit - logit * target + max_val + \\\n",
    "            ((-max_val).exp() + (-logit - max_val).exp()).log()\n",
    "\n",
    "        invprobs2 = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n",
    "        loss2 = (invprobs2 * self.gamma).exp() * loss2\n",
    "        if len(loss2.size()) == 2:\n",
    "            loss2 = loss2.sum(dim=1)\n",
    "        \n",
    "        return (loss * mix_info['lam'] + loss2 * (1 - mix_info['lam'])).mean()\n",
    "\n",
    "\n",
    "class ImprovedPANNsLoss(nn.Module):\n",
    "    def __init__(self, output_key=\"logit\", weights=[1, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_key = output_key\n",
    "        if output_key == \"logit\":\n",
    "            self.normal_loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.normal_loss = nn.BCELoss()\n",
    "\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[self.output_key]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_output\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        normal_loss = self.normal_loss(input_, target)\n",
    "        auxiliary_loss = self.bce(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss\n",
    "\n",
    "\n",
    "class ImprovedFocalLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = FocalLoss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target, mixup_info=None):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_logit\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "        \n",
    "        if mixup_info is None:\n",
    "            normal_loss = self.focal(input_, target)\n",
    "            auxiliary_loss = self.focal(clipwise_output_with_max, target)\n",
    "        else:\n",
    "            normal_loss = self.focal(input_, target, mixup_info)\n",
    "            auxiliary_loss = self.focal(clipwise_output_with_max, target, mixup_info)\n",
    "\n",
    "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRAP. Instance-level average\n",
    "# Assume float preds [BxC], labels [BxC] of 0 or 1\n",
    "def LRAP(preds, labels):\n",
    "    # Ranks of the predictions\n",
    "    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n",
    "    # i, j corresponds to rank of prediction in row i\n",
    "    class_ranks = torch.zeros_like(ranked_classes)\n",
    "    for i in range(ranked_classes.size(0)):\n",
    "        for j in range(ranked_classes.size(1)):\n",
    "            class_ranks[i, ranked_classes[i][j]] = j + 1\n",
    "    # Mask out to only use the ranks of relevant GT labels\n",
    "    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n",
    "    # All the GT ranks are in front now\n",
    "    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n",
    "    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n",
    "    score_matrix = pos_matrix / sorted_ground_truth_ranks\n",
    "    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n",
    "    scores = score_matrix * score_mask_matrix\n",
    "    score = (scores.sum(-1) / labels.sum(-1)).mean()\n",
    "    return score.item()\n",
    "\n",
    "# label-level average\n",
    "# Assume float preds [BxC], labels [BxC] of 0 or 1\n",
    "def LWLRAP(preds, labels):\n",
    "    # Ranks of the predictions\n",
    "    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n",
    "    # i, j corresponds to rank of prediction in row i\n",
    "    class_ranks = torch.zeros_like(ranked_classes)\n",
    "    for i in range(ranked_classes.size(0)):\n",
    "        for j in range(ranked_classes.size(1)):\n",
    "            class_ranks[i, ranked_classes[i][j]] = j + 1\n",
    "    # Mask out to only use the ranks of relevant GT labels\n",
    "    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n",
    "    # All the GT ranks are in front now\n",
    "    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n",
    "    # Number of GT labels per instance\n",
    "    num_labels = labels.sum(-1)\n",
    "    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n",
    "    score_matrix = pos_matrix / sorted_ground_truth_ranks\n",
    "    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n",
    "    scores = score_matrix * score_mask_matrix\n",
    "    score = scores.sum() / labels.sum()\n",
    "    return score.item()\n",
    "\n",
    "def mixup_socre(cor, x, y, mix_info):\n",
    "    return (cor(x, y) * mix_info['lam'] + cor(x, y[mix_info['index']].squeeze()) * (1-mix_info['lam'])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NAME = 'efficient-b0-sed-powd'\n",
    "\n",
    "n_splits = 5\n",
    "random_state = 1\n",
    "epochs = 35\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "spectrogram_params = {\n",
    "    'n_fft': 2048,\n",
    "    'hop_length': 512,\n",
    "    'win_length': 2048,\n",
    "    'window': 'hann',\n",
    "    'center': True,\n",
    "    'pad_mode': 'reflect',\n",
    "    'freeze_parameters': True\n",
    "}\n",
    "\n",
    "logmel_extractor_params = {\n",
    "    'sr': SR,\n",
    "    'n_fft': 2048,\n",
    "    'n_mels': 256,\n",
    "    'fmin': 0,\n",
    "    'fmax': 15000,\n",
    "    'ref' : 1.0,\n",
    "    'amin': 1e-10,\n",
    "    'top_db': None,\n",
    "    'is_log': False,\n",
    "    'freeze_parameters': True\n",
    "}\n",
    "\n",
    "spec_augmenter_params = {\n",
    "    'time_drop_width':  64,\n",
    "    'time_stripes_num': 2,\n",
    "    'freq_drop_width':  8,\n",
    "    'freq_stripes_num': 2\n",
    "}\n",
    "\n",
    "pce_params = {\n",
    "    'gain': 0.98,\n",
    "    'bias': 2,\n",
    "    'power': 0.5,\n",
    "    'time_constant': 0.4,\n",
    "    'eps': 0.000001,\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'base_model_name': 'efficientnet-b1',\n",
    "    'pretrained': True,\n",
    "    'num_classes': 24,\n",
    "    'spectrogram_params': spectrogram_params,\n",
    "    'logmel_extractor_params': logmel_extractor_params,\n",
    "    'spec_augmenter_params': spec_augmenter_params,\n",
    "    'pce_params': pce_params\n",
    "}\n",
    "\n",
    "\n",
    "optim_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 5e-5,\n",
    "    'betas': (0.9, 0.999)\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "    'mode': 'max',\n",
    "    'patience': 1,\n",
    "    'factor': 0.6,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "pcen_parameters = {\n",
    "    'gain': 0.98,\n",
    "    'bias': 2,\n",
    "    'power': 0.5,\n",
    "    'time_constant': 0.4,\n",
    "    'eps': 0.000001,\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'pcen_parameters': pcen_parameters,\n",
    "    'pre_calc': False,\n",
    "    'train': True,\n",
    "    'data_path': '/home/yuigahama/kaggle/rfcx/data/train/'  \n",
    "}\n",
    "\n",
    "val_params = {\n",
    "    'train': True,\n",
    "    'pre_calc': True,\n",
    "    'pcen_parameters': pcen_parameters,\n",
    "    'data_path': '/home/yuigahama/kaggle/rfcx/data/train/'\n",
    "}\n",
    "\n",
    "test_data_params = {\n",
    "    'train': False,\n",
    "    'path': '/home/yuigahama/kaggle/rfcx/data/test_wo_fp/'\n",
    "}\n",
    "\n",
    "dataloder_params = {\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 15,\n",
    "    'pin_memory': False,\n",
    "}\n",
    "\n",
    "test_dataloder_params = {\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 15,\n",
    "    'pin_memory': False,\n",
    "    'shuffle':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- fold 0 ----------\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "E   1| T | L: 3.92 | S: 0.179 | C: 50/972 | V | L: 2.53 | S: 0.168 | C: 11/244 | T: 0:00:37 | es: 50\n",
      "E   2| T | L: 2.05 | S: 0.202 | C: 53/972 | V | L: 3.32 | S: 0.228 | C: 21/244 | T: 0:00:36 | es: 50\n",
      "E   3| T | L: 1.95 | S: 0.208 | C: 51/972 | V | L: 2.94 | S: 0.286 | C: 35/244 | T: 0:00:36 | es: 50\n",
      "E   4| T | L: 1.9 | S: 0.223 | C: 55/972 | V | L: 2.22 | S: 0.257 | C: 22/244 | T: 0:00:36 | es: 50\n",
      "E   5| T | L: 1.86 | S: 0.238 | C: 55/972 | V | L: 1.75 | S: 0.327 | C: 32/244 | T: 0:00:36 | es: 49\n",
      "E   6| T | L: 1.84 | S: 0.251 | C: 55/972 | V | L: 1.77 | S: 0.327 | C: 43/244 | T: 0:00:36 | es: 49\n",
      "E   7| T | L: 1.83 | S: 0.266 | C: 58/972 | V | L: 1.63 | S: 0.351 | C: 41/244 | T: 0:00:36 | es: 49\n",
      "E   8| T | L: 1.79 | S: 0.295 | C: 60/972 | V | L: 1.55 | S: 0.404 | C: 56/244 | T: 0:00:36 | es: 49\n",
      "E   9| T | L: 1.77 | S: 0.304 | C: 60/972 | V | L: 1.53 | S: 0.417 | C: 61/244 | T: 0:00:36 | es: 49\n",
      "E  10| T | L: 1.75 | S: 0.335 | C: 60/972 | V | L: 1.41 | S: 0.509 | C: 88/244 | T: 0:00:36 | es: 49\n",
      "E  11| T | L: 1.75 | S: 0.334 | C: 61/972 | V | L: 1.44 | S: 0.543 | C: 92/244 | T: 0:00:36 | es: 49\n",
      "E  12| T | L: 1.73 | S: 0.354 | C: 60/972 | V | L: 1.37 | S: 0.563 | C: 97/244 | T: 0:00:36 | es: 49\n",
      "E  13| T | L: 1.7 | S: 0.383 | C: 61/972 | V | L: 1.36 | S: 0.566 | C: 100/244 | T: 0:00:36 | es: 49\n",
      "E  14| T | L: 1.71 | S: 0.38 | C: 61/972 | V | L: 1.32 | S: 0.599 | C: 107/244 | T: 0:00:36 | es: 49\n",
      "E  15| T | L: 1.66 | S: 0.414 | C: 61/972 | V | L: 1.31 | S: 0.617 | C: 108/244 | T: 0:00:36 | es: 49\n",
      "E  16| T | L: 1.67 | S: 0.42 | C: 61/972 | V | L: 1.3 | S: 0.603 | C: 105/244 | T: 0:00:36 | es: 49\n",
      "E  17| T | L: 1.63 | S: 0.446 | C: 61/972 | V | L: 1.17 | S: 0.7 | C: 140/244 | T: 0:00:36 | es: 48\n",
      "E  18| T | L: 1.64 | S: 0.46 | C: 61/972 | V | L: 1.23 | S: 0.676 | C: 131/244 | T: 0:00:35 | es: 48\n",
      "E  19| T | L: 1.6 | S: 0.457 | C: 61/972 | V | L: 1.19 | S: 0.665 | C: 125/244 | T: 0:00:36 | es: 47\n",
      "E  20| T | L: 1.64 | S: 0.464 | C: 61/972 | V | L: 1.21 | S: 0.694 | C: 135/244 | T: 0:00:36 | es: 46\n",
      "E  21| T | L: 1.61 | S: 0.473 | C: 61/972 | V | L: 1.16 | S: 0.712 | C: 139/244 | T: 0:00:36 | es: 45\n",
      "E  22| T | L: 1.59 | S: 0.486 | C: 61/972 | V | L: 1.09 | S: 0.735 | C: 146/244 | T: 0:00:36 | es: 45\n",
      "E  23| T | L: 1.57 | S: 0.503 | C: 61/972 | V | L: 1.11 | S: 0.729 | C: 143/244 | T: 0:00:36 | es: 45\n",
      "E  24| T | L: 1.58 | S: 0.491 | C: 61/972 | V | L: 1.16 | S: 0.724 | C: 143/244 | T: 0:00:36 | es: 44\n",
      "E  25| T | L: 1.56 | S: 0.5 | C: 61/972 | V | L: 1.11 | S: 0.757 | C: 154/244 | T: 0:00:36 | es: 43\n",
      "E  26| T | L: 1.51 | S: 0.521 | C: 61/972 | V | L: 1.09 | S: 0.769 | C: 158/244 | T: 0:00:36 | es: 43\n",
      "E  27| T | L: 1.55 | S: 0.516 | C: 61/972 | V | L: 1.08 | S: 0.759 | C: 158/244 | T: 0:00:36 | es: 43\n",
      "E  28| T | L: 1.56 | S: 0.508 | C: 61/972 | V | L: 1.08 | S: 0.755 | C: 154/244 | T: 0:00:36 | es: 42\n",
      "E  29| T | L: 1.54 | S: 0.526 | C: 61/972 | V | L: 1.02 | S: 0.769 | C: 160/244 | T: 0:00:36 | es: 41\n",
      "E  30| T | L: 1.54 | S: 0.51 | C: 61/972 | V | L: 1.11 | S: 0.772 | C: 158/244 | T: 0:00:36 | es: 41\n",
      "E  31| T | L: 1.53 | S: 0.517 | C: 61/972 | V | L: 1.05 | S: 0.757 | C: 148/244 | T: 0:00:36 | es: 41\n",
      "E  32| T | L: 1.51 | S: 0.526 | C: 61/972 | V | L: 1.06 | S: 0.772 | C: 159/244 | T: 0:00:36 | es: 40\n",
      "E  33| T | L: 1.53 | S: 0.522 | C: 61/972 | V | L: 1.02 | S: 0.771 | C: 159/244 | T: 0:00:36 | es: 40\n",
      "E  34| T | L: 1.52 | S: 0.535 | C: 61/972 | V | L: 0.993 | S: 0.76 | C: 154/244 | T: 0:00:36 | es: 39\n",
      "E  35| T | L: 1.5 | S: 0.538 | C: 61/972 | V | L: 1.03 | S: 0.769 | C: 155/244 | T: 0:00:36 | es: 38\n",
      "E  36| T | L: 1.47 | S: 0.543 | C: 61/972 | V | L: 1.02 | S: 0.783 | C: 163/244 | T: 0:00:36 | es: 37\n",
      "E  37| T | L: 1.52 | S: 0.553 | C: 61/972 | V | L: 1.05 | S: 0.735 | C: 145/244 | T: 0:00:36 | es: 37\n",
      "E  38| T | L: 1.49 | S: 0.563 | C: 61/972 | V | L: 0.971 | S: 0.802 | C: 169/244 | T: 0:00:36 | es: 36\n",
      "E  39| T | L: 1.51 | S: 0.552 | C: 61/972 | V | L: 1.02 | S: 0.769 | C: 154/244 | T: 0:00:36 | es: 36\n",
      "E  40| T | L: 1.49 | S: 0.548 | C: 61/972 | V | L: 1.05 | S: 0.793 | C: 167/244 | T: 0:00:36 | es: 35\n",
      "E  41| T | L: 1.48 | S: 0.549 | C: 61/972 | V | L: 0.955 | S: 0.775 | C: 156/244 | T: 0:00:36 | es: 34\n",
      "E  42| T | L: 1.53 | S: 0.549 | C: 61/972 | V | L: 1.06 | S: 0.794 | C: 170/244 | T: 0:00:36 | es: 33\n",
      "E  43| T | L: 1.52 | S: 0.554 | C: 61/972 | V | L: 1.01 | S: 0.786 | C: 164/244 | T: 0:00:36 | es: 32\n",
      "E  44| T | L: 1.51 | S: 0.542 | C: 61/972 | V | L: 0.976 | S: 0.804 | C: 167/244 | T: 0:00:36 | es: 31\n",
      "E  45| T | L: 1.45 | S: 0.542 | C: 61/972 | V | L: 0.925 | S: 0.817 | C: 175/244 | T: 0:00:36 | es: 31\n",
      "E  46| T | L: 1.46 | S: 0.547 | C: 61/972 | V | L: 0.985 | S: 0.785 | C: 162/244 | T: 0:00:36 | es: 31\n",
      "E  47| T | L: 1.47 | S: 0.558 | C: 61/972 | V | L: 0.984 | S: 0.789 | C: 159/244 | T: 0:00:37 | es: 30\n",
      "E  48| T | L: 1.49 | S: 0.551 | C: 61/972 | V | L: 1.06 | S: 0.762 | C: 161/244 | T: 0:00:36 | es: 29\n",
      "E  49| T | L: 1.46 | S: 0.539 | C: 61/972 | V | L: 0.943 | S: 0.814 | C: 168/244 | T: 0:00:36 | es: 28\n",
      "E  50| T | L: 1.46 | S: 0.566 | C: 61/972 | V | L: 0.942 | S: 0.797 | C: 170/244 | T: 0:00:36 | es: 27\n",
      "E  51| T | L: 1.49 | S: 0.566 | C: 61/972 | V | L: 0.955 | S: 0.776 | C: 158/244 | T: 0:00:37 | es: 26\n",
      "E  52| T | L: 1.49 | S: 0.552 | C: 61/972 | V | L: 0.985 | S: 0.787 | C: 163/244 | T: 0:00:36 | es: 25\n",
      "E  53| T | L: 1.48 | S: 0.566 | C: 61/972 | V | L: 1.01 | S: 0.78 | C: 161/244 | T: 0:00:36 | es: 24\n",
      "E  54| T | L: 1.5 | S: 0.546 | C: 61/972 | V | L: 1.05 | S: 0.78 | C: 160/244 | T: 0:00:36 | es: 23\n",
      "E  55| T | L: 1.45 | S: 0.561 | C: 61/972 | V | L: 0.916 | S: 0.803 | C: 169/244 | T: 0:00:36 | es: 22\n",
      "E  56| T | L: 1.45 | S: 0.567 | C: 61/972 | V | L: 0.984 | S: 0.833 | C: 181/244 | T: 0:00:37 | es: 21\n",
      "E  57| T | L: 1.48 | S: 0.566 | C: 61/972 | V | L: 1.02 | S: 0.795 | C: 169/244 | T: 0:00:36 | es: 21\n",
      "E  58| T | L: 1.47 | S: 0.565 | C: 61/972 | V | L: 0.955 | S: 0.782 | C: 156/244 | T: 0:00:36 | es: 20\n",
      "E  59| T | L: 1.46 | S: 0.566 | C: 61/972 | V | L: 0.905 | S: 0.788 | C: 163/244 | T: 0:00:36 | es: 19\n",
      "E  60| T | L: 1.46 | S: 0.565 | C: 61/972 | V | L: 0.99 | S: 0.783 | C: 160/244 | T: 0:00:36 | es: 18\n",
      "E  61| T | L: 1.46 | S: 0.557 | C: 61/972 | V | L: 0.966 | S: 0.804 | C: 173/244 | T: 0:00:36 | es: 17\n",
      "E  62| T | L: 1.5 | S: 0.556 | C: 61/972 | V | L: 1.0 | S: 0.791 | C: 165/244 | T: 0:00:36 | es: 16\n",
      "E  63| T | L: 1.45 | S: 0.569 | C: 61/972 | V | L: 0.905 | S: 0.817 | C: 172/244 | T: 0:00:36 | es: 15\n",
      "E  64| T | L: 1.46 | S: 0.56 | C: 61/972 | V | L: 0.966 | S: 0.833 | C: 179/244 | T: 0:00:36 | es: 14\n",
      "E  65| T | L: 1.42 | S: 0.567 | C: 61/972 | V | L: 0.912 | S: 0.798 | C: 163/244 | T: 0:00:36 | es: 14\n",
      "E  66| T | L: 1.49 | S: 0.549 | C: 61/972 | V | L: 0.966 | S: 0.802 | C: 164/244 | T: 0:00:36 | es: 13\n",
      "E  67| T | L: 1.44 | S: 0.567 | C: 61/972 | V | L: 1.02 | S: 0.801 | C: 168/244 | T: 0:00:36 | es: 12\n",
      "E  68| T | L: 1.43 | S: 0.564 | C: 61/972 | V | L: 0.985 | S: 0.798 | C: 167/244 | T: 0:00:36 | es: 11\n",
      "E  69| T | L: 1.47 | S: 0.56 | C: 61/972 | V | L: 0.936 | S: 0.822 | C: 176/244 | T: 0:00:36 | es: 10\n",
      "E  70| T | L: 1.5 | S: 0.571 | C: 61/972 | V | L: 1.02 | S: 0.785 | C: 160/244 | T: 0:00:36 | es: 9\n",
      "E  71| T | L: 1.51 | S: 0.566 | C: 61/972 | V | L: 1.02 | S: 0.808 | C: 175/244 | T: 0:00:36 | es: 8\n",
      "E  72| T | L: 1.44 | S: 0.561 | C: 61/972 | V | L: 1.03 | S: 0.773 | C: 162/244 | T: 0:00:36 | es: 7\n",
      "E  73| T | L: 1.43 | S: 0.576 | C: 61/972 | V | L: 0.976 | S: 0.794 | C: 162/244 | T: 0:00:36 | es: 6\n",
      "E  74| T | L: 1.42 | S: 0.588 | C: 61/972 | V | L: 0.945 | S: 0.789 | C: 166/244 | T: 0:00:36 | es: 5\n",
      "bast score: 0.8334040194749832\n",
      "---------- fold 1 ----------\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "E   1| T | L: 3.79 | S: 0.178 | C: 53/973 | V | L: 2.54 | S: 0.183 | C: 14/243 | T: 0:00:36 | es: 50\n",
      "E   2| T | L: 2.08 | S: 0.196 | C: 54/973 | V | L: 2.6 | S: 0.232 | C: 24/243 | T: 0:00:36 | es: 50\n",
      "E   3| T | L: 1.93 | S: 0.23 | C: 55/973 | V | L: 2.02 | S: 0.305 | C: 35/243 | T: 0:00:35 | es: 50\n",
      "E   4| T | L: 1.9 | S: 0.229 | C: 55/973 | V | L: 1.84 | S: 0.317 | C: 37/243 | T: 0:00:36 | es: 50\n",
      "E   5| T | L: 1.87 | S: 0.244 | C: 60/973 | V | L: 1.65 | S: 0.388 | C: 55/243 | T: 0:00:36 | es: 50\n",
      "E   6| T | L: 1.82 | S: 0.278 | C: 61/973 | V | L: 1.57 | S: 0.424 | C: 61/243 | T: 0:00:36 | es: 50\n",
      "E   7| T | L: 1.8 | S: 0.297 | C: 61/973 | V | L: 1.47 | S: 0.449 | C: 68/243 | T: 0:00:36 | es: 50\n",
      "E   8| T | L: 1.79 | S: 0.309 | C: 61/973 | V | L: 1.54 | S: 0.446 | C: 65/243 | T: 0:00:36 | es: 50\n",
      "E   9| T | L: 1.75 | S: 0.339 | C: 61/973 | V | L: 1.39 | S: 0.532 | C: 93/243 | T: 0:00:36 | es: 49\n",
      "E  10| T | L: 1.76 | S: 0.345 | C: 61/973 | V | L: 1.39 | S: 0.527 | C: 92/243 | T: 0:00:36 | es: 49\n",
      "E  11| T | L: 1.73 | S: 0.368 | C: 61/973 | V | L: 1.39 | S: 0.547 | C: 90/243 | T: 0:00:36 | es: 48\n",
      "E  12| T | L: 1.71 | S: 0.375 | C: 61/973 | V | L: 1.24 | S: 0.649 | C: 129/243 | T: 0:00:36 | es: 48\n",
      "E  13| T | L: 1.69 | S: 0.396 | C: 61/973 | V | L: 1.32 | S: 0.637 | C: 120/243 | T: 0:00:36 | es: 48\n",
      "E  14| T | L: 1.69 | S: 0.397 | C: 61/973 | V | L: 1.23 | S: 0.649 | C: 121/243 | T: 0:00:36 | es: 47\n",
      "E  15| T | L: 1.68 | S: 0.419 | C: 61/973 | V | L: 1.22 | S: 0.647 | C: 124/243 | T: 0:00:37 | es: 46\n",
      "E  16| T | L: 1.66 | S: 0.437 | C: 61/973 | V | L: 1.22 | S: 0.69 | C: 141/243 | T: 0:00:36 | es: 45\n",
      "E  17| T | L: 1.63 | S: 0.46 | C: 61/973 | V | L: 1.14 | S: 0.71 | C: 146/243 | T: 0:00:36 | es: 45\n",
      "E  18| T | L: 1.62 | S: 0.455 | C: 61/973 | V | L: 1.15 | S: 0.694 | C: 139/243 | T: 0:00:36 | es: 45\n",
      "E  19| T | L: 1.6 | S: 0.474 | C: 61/973 | V | L: 1.14 | S: 0.754 | C: 149/243 | T: 0:00:36 | es: 44\n",
      "E  20| T | L: 1.63 | S: 0.458 | C: 61/973 | V | L: 1.12 | S: 0.712 | C: 147/243 | T: 0:00:36 | es: 44\n",
      "E  21| T | L: 1.64 | S: 0.472 | C: 61/973 | V | L: 1.19 | S: 0.765 | C: 157/243 | T: 0:00:36 | es: 43\n",
      "E  22| T | L: 1.59 | S: 0.492 | C: 61/973 | V | L: 1.01 | S: 0.764 | C: 157/243 | T: 0:00:37 | es: 43\n",
      "E  23| T | L: 1.55 | S: 0.505 | C: 61/973 | V | L: 1.04 | S: 0.745 | C: 154/243 | T: 0:00:38 | es: 42\n",
      "E  24| T | L: 1.56 | S: 0.501 | C: 61/973 | V | L: 1.04 | S: 0.803 | C: 167/243 | T: 0:00:37 | es: 41\n",
      "E  25| T | L: 1.57 | S: 0.505 | C: 61/973 | V | L: 0.982 | S: 0.808 | C: 174/243 | T: 0:00:36 | es: 41\n",
      "E  26| T | L: 1.54 | S: 0.52 | C: 61/973 | V | L: 0.974 | S: 0.773 | C: 163/243 | T: 0:00:36 | es: 41\n",
      "E  27| T | L: 1.54 | S: 0.519 | C: 61/973 | V | L: 1.11 | S: 0.768 | C: 165/243 | T: 0:00:36 | es: 40\n",
      "E  28| T | L: 1.59 | S: 0.504 | C: 61/973 | V | L: 1.14 | S: 0.772 | C: 163/243 | T: 0:00:36 | es: 39\n",
      "E  29| T | L: 1.52 | S: 0.513 | C: 61/973 | V | L: 0.855 | S: 0.823 | C: 177/243 | T: 0:00:36 | es: 38\n",
      "E  30| T | L: 1.56 | S: 0.513 | C: 61/973 | V | L: 1.13 | S: 0.781 | C: 169/243 | T: 0:00:37 | es: 38\n",
      "E  31| T | L: 1.54 | S: 0.521 | C: 61/973 | V | L: 1.05 | S: 0.789 | C: 166/243 | T: 0:00:36 | es: 37\n",
      "E  32| T | L: 1.53 | S: 0.524 | C: 61/973 | V | L: 1.02 | S: 0.773 | C: 164/243 | T: 0:00:36 | es: 36\n",
      "E  33| T | L: 1.56 | S: 0.522 | C: 61/973 | V | L: 1.0 | S: 0.812 | C: 171/243 | T: 0:00:36 | es: 35\n",
      "E  34| T | L: 1.53 | S: 0.52 | C: 61/973 | V | L: 0.949 | S: 0.787 | C: 163/243 | T: 0:00:36 | es: 34\n",
      "E  35| T | L: 1.53 | S: 0.532 | C: 61/973 | V | L: 0.941 | S: 0.803 | C: 168/243 | T: 0:00:37 | es: 33\n",
      "E  36| T | L: 1.49 | S: 0.536 | C: 61/973 | V | L: 0.954 | S: 0.833 | C: 183/243 | T: 0:00:36 | es: 32\n",
      "E  37| T | L: 1.54 | S: 0.522 | C: 61/973 | V | L: 1.03 | S: 0.802 | C: 168/243 | T: 0:00:36 | es: 32\n",
      "E  38| T | L: 1.5 | S: 0.555 | C: 61/973 | V | L: 0.998 | S: 0.811 | C: 175/243 | T: 0:00:36 | es: 31\n",
      "E  39| T | L: 1.5 | S: 0.533 | C: 61/973 | V | L: 0.935 | S: 0.811 | C: 173/243 | T: 0:00:36 | es: 30\n",
      "E  40| T | L: 1.52 | S: 0.543 | C: 61/973 | V | L: 0.965 | S: 0.861 | C: 187/243 | T: 0:00:36 | es: 29\n",
      "E  41| T | L: 1.47 | S: 0.559 | C: 61/973 | V | L: 0.874 | S: 0.85 | C: 186/243 | T: 0:00:36 | es: 29\n",
      "E  42| T | L: 1.54 | S: 0.533 | C: 61/973 | V | L: 0.991 | S: 0.793 | C: 172/243 | T: 0:00:36 | es: 28\n",
      "E  43| T | L: 1.49 | S: 0.541 | C: 61/973 | V | L: 0.907 | S: 0.835 | C: 176/243 | T: 0:00:36 | es: 27\n",
      "E  44| T | L: 1.51 | S: 0.55 | C: 61/973 | V | L: 0.962 | S: 0.83 | C: 179/243 | T: 0:00:36 | es: 26\n",
      "E  45| T | L: 1.47 | S: 0.544 | C: 61/973 | V | L: 0.906 | S: 0.807 | C: 173/243 | T: 0:00:36 | es: 25\n",
      "E  46| T | L: 1.47 | S: 0.547 | C: 61/973 | V | L: 0.919 | S: 0.815 | C: 175/243 | T: 0:00:36 | es: 24\n",
      "E  47| T | L: 1.5 | S: 0.544 | C: 61/973 | V | L: 0.949 | S: 0.828 | C: 179/243 | T: 0:00:37 | es: 23\n",
      "E  48| T | L: 1.47 | S: 0.558 | C: 61/973 | V | L: 1.03 | S: 0.782 | C: 166/243 | T: 0:00:36 | es: 22\n",
      "E  49| T | L: 1.5 | S: 0.54 | C: 61/973 | V | L: 0.948 | S: 0.797 | C: 170/243 | T: 0:00:36 | es: 21\n",
      "E  50| T | L: 1.48 | S: 0.531 | C: 61/973 | V | L: 0.871 | S: 0.822 | C: 181/243 | T: 0:00:36 | es: 20\n",
      "E  51| T | L: 1.48 | S: 0.569 | C: 61/973 | V | L: 0.897 | S: 0.841 | C: 184/243 | T: 0:00:36 | es: 19\n",
      "E  52| T | L: 1.51 | S: 0.552 | C: 61/973 | V | L: 0.972 | S: 0.81 | C: 180/243 | T: 0:00:36 | es: 18\n",
      "E  53| T | L: 1.5 | S: 0.545 | C: 61/973 | V | L: 0.995 | S: 0.793 | C: 174/243 | T: 0:00:36 | es: 17\n",
      "E  54| T | L: 1.53 | S: 0.548 | C: 61/973 | V | L: 1.05 | S: 0.826 | C: 184/243 | T: 0:00:36 | es: 16\n",
      "E  55| T | L: 1.43 | S: 0.562 | C: 61/973 | V | L: 0.928 | S: 0.811 | C: 174/243 | T: 0:00:36 | es: 15\n",
      "E  56| T | L: 1.49 | S: 0.552 | C: 61/973 | V | L: 0.984 | S: 0.817 | C: 179/243 | T: 0:00:36 | es: 14\n",
      "E  57| T | L: 1.46 | S: 0.559 | C: 61/973 | V | L: 0.936 | S: 0.813 | C: 175/243 | T: 0:00:36 | es: 13\n",
      "E  58| T | L: 1.45 | S: 0.561 | C: 61/973 | V | L: 0.917 | S: 0.806 | C: 175/243 | T: 0:00:36 | es: 12\n",
      "E  59| T | L: 1.45 | S: 0.56 | C: 61/973 | V | L: 0.875 | S: 0.83 | C: 181/243 | T: 0:00:36 | es: 11\n",
      "E  60| T | L: 1.48 | S: 0.548 | C: 61/973 | V | L: 0.985 | S: 0.841 | C: 186/243 | T: 0:00:36 | es: 10\n",
      "E  61| T | L: 1.43 | S: 0.569 | C: 61/973 | V | L: 0.927 | S: 0.798 | C: 168/243 | T: 0:00:36 | es: 9\n",
      "E  62| T | L: 1.5 | S: 0.55 | C: 61/973 | V | L: 0.882 | S: 0.817 | C: 171/243 | T: 0:00:36 | es: 8\n",
      "E  63| T | L: 1.48 | S: 0.544 | C: 61/973 | V | L: 0.927 | S: 0.809 | C: 175/243 | T: 0:00:36 | es: 7\n",
      "E  64| T | L: 1.48 | S: 0.555 | C: 61/973 | V | L: 0.877 | S: 0.832 | C: 185/243 | T: 0:00:36 | es: 6\n",
      "E  65| T | L: 1.45 | S: 0.561 | C: 61/973 | V | L: 0.828 | S: 0.83 | C: 179/243 | T: 0:00:36 | es: 5\n",
      "E  66| T | L: 1.46 | S: 0.557 | C: 61/973 | V | L: 1.02 | S: 0.829 | C: 183/243 | T: 0:00:36 | es: 4\n",
      "E  67| T | L: 1.43 | S: 0.562 | C: 61/973 | V | L: 0.921 | S: 0.818 | C: 178/243 | T: 0:00:36 | es: 3\n",
      "E  68| T | L: 1.46 | S: 0.556 | C: 61/973 | V | L: 0.885 | S: 0.816 | C: 180/243 | T: 0:00:36 | es: 2\n",
      "E  69| T | L: 1.47 | S: 0.568 | C: 61/973 | V | L: 0.885 | S: 0.863 | C: 192/243 | T: 0:00:36 | es: 1\n",
      "E  70| T | L: 1.5 | S: 0.563 | C: 61/973 | V | L: 0.982 | S: 0.811 | C: 174/243 | T: 0:00:36 | es: 1\n",
      "bast score: 0.8631232790648937\n",
      "---------- fold 2 ----------\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "E   1| T | L: 4.23 | S: 0.185 | C: 49/973 | V | L: 2.72 | S: 0.169 | C: 13/243 | T: 0:00:36 | es: 50\n",
      "E   2| T | L: 2.08 | S: 0.2 | C: 52/973 | V | L: 2.51 | S: 0.222 | C: 23/243 | T: 0:00:36 | es: 50\n",
      "E   3| T | L: 1.95 | S: 0.22 | C: 57/973 | V | L: 2.25 | S: 0.272 | C: 29/243 | T: 0:00:36 | es: 50\n",
      "E   4| T | L: 1.91 | S: 0.232 | C: 55/973 | V | L: 2.02 | S: 0.294 | C: 38/243 | T: 0:00:36 | es: 50\n",
      "E   5| T | L: 1.87 | S: 0.25 | C: 59/973 | V | L: 1.73 | S: 0.332 | C: 35/243 | T: 0:00:36 | es: 50\n",
      "E   6| T | L: 1.83 | S: 0.275 | C: 58/973 | V | L: 1.67 | S: 0.395 | C: 61/243 | T: 0:00:36 | es: 50\n",
      "E   7| T | L: 1.81 | S: 0.288 | C: 60/973 | V | L: 1.54 | S: 0.448 | C: 70/243 | T: 0:00:36 | es: 50\n",
      "E   8| T | L: 1.78 | S: 0.304 | C: 61/973 | V | L: 1.46 | S: 0.503 | C: 86/243 | T: 0:00:36 | es: 50\n",
      "E   9| T | L: 1.77 | S: 0.315 | C: 61/973 | V | L: 1.46 | S: 0.486 | C: 79/243 | T: 0:00:36 | es: 50\n",
      "E  10| T | L: 1.73 | S: 0.347 | C: 61/973 | V | L: 1.39 | S: 0.542 | C: 100/243 | T: 0:00:36 | es: 49\n",
      "E  11| T | L: 1.72 | S: 0.355 | C: 61/973 | V | L: 1.44 | S: 0.54 | C: 101/243 | T: 0:00:36 | es: 49\n",
      "E  12| T | L: 1.71 | S: 0.373 | C: 61/973 | V | L: 1.34 | S: 0.551 | C: 96/243 | T: 0:00:36 | es: 48\n",
      "E  13| T | L: 1.69 | S: 0.379 | C: 61/973 | V | L: 1.37 | S: 0.574 | C: 107/243 | T: 0:00:36 | es: 48\n",
      "E  14| T | L: 1.71 | S: 0.398 | C: 61/973 | V | L: 1.33 | S: 0.597 | C: 115/243 | T: 0:00:36 | es: 48\n",
      "E  15| T | L: 1.7 | S: 0.398 | C: 61/973 | V | L: 1.32 | S: 0.607 | C: 108/243 | T: 0:00:36 | es: 48\n",
      "E  16| T | L: 1.67 | S: 0.411 | C: 61/973 | V | L: 1.34 | S: 0.59 | C: 111/243 | T: 0:00:36 | es: 48\n",
      "E  17| T | L: 1.65 | S: 0.419 | C: 60/973 | V | L: 1.28 | S: 0.644 | C: 125/243 | T: 0:00:36 | es: 47\n",
      "E  18| T | L: 1.64 | S: 0.424 | C: 61/973 | V | L: 1.2 | S: 0.689 | C: 133/243 | T: 0:00:36 | es: 47\n",
      "E  19| T | L: 1.64 | S: 0.435 | C: 61/973 | V | L: 1.3 | S: 0.673 | C: 127/243 | T: 0:00:36 | es: 47\n",
      "E  20| T | L: 1.64 | S: 0.437 | C: 61/973 | V | L: 1.32 | S: 0.637 | C: 123/243 | T: 0:00:36 | es: 46\n",
      "E  21| T | L: 1.62 | S: 0.455 | C: 61/973 | V | L: 1.23 | S: 0.634 | C: 116/243 | T: 0:00:36 | es: 45\n",
      "E  22| T | L: 1.61 | S: 0.453 | C: 61/973 | V | L: 1.15 | S: 0.736 | C: 141/243 | T: 0:00:36 | es: 44\n",
      "E  23| T | L: 1.62 | S: 0.461 | C: 61/973 | V | L: 1.16 | S: 0.779 | C: 165/243 | T: 0:00:36 | es: 44\n",
      "E  24| T | L: 1.59 | S: 0.477 | C: 61/973 | V | L: 1.15 | S: 0.739 | C: 149/243 | T: 0:00:36 | es: 44\n",
      "E  25| T | L: 1.59 | S: 0.471 | C: 61/973 | V | L: 1.12 | S: 0.719 | C: 147/243 | T: 0:00:36 | es: 43\n",
      "E  26| T | L: 1.58 | S: 0.472 | C: 61/973 | V | L: 1.1 | S: 0.714 | C: 141/243 | T: 0:00:36 | es: 42\n",
      "E  27| T | L: 1.62 | S: 0.477 | C: 61/973 | V | L: 1.13 | S: 0.725 | C: 145/243 | T: 0:00:36 | es: 41\n",
      "E  28| T | L: 1.6 | S: 0.501 | C: 61/973 | V | L: 1.12 | S: 0.766 | C: 159/243 | T: 0:00:36 | es: 40\n",
      "E  29| T | L: 1.56 | S: 0.487 | C: 61/973 | V | L: 1.1 | S: 0.737 | C: 151/243 | T: 0:00:36 | es: 39\n",
      "E  30| T | L: 1.57 | S: 0.502 | C: 61/973 | V | L: 1.21 | S: 0.766 | C: 160/243 | T: 0:00:36 | es: 38\n",
      "E  31| T | L: 1.55 | S: 0.496 | C: 61/973 | V | L: 0.997 | S: 0.793 | C: 164/243 | T: 0:00:36 | es: 37\n",
      "E  32| T | L: 1.55 | S: 0.499 | C: 61/973 | V | L: 1.07 | S: 0.782 | C: 163/243 | T: 0:00:36 | es: 37\n",
      "E  33| T | L: 1.57 | S: 0.512 | C: 61/973 | V | L: 1.05 | S: 0.777 | C: 157/243 | T: 0:00:36 | es: 36\n",
      "E  34| T | L: 1.53 | S: 0.512 | C: 61/973 | V | L: 1.0 | S: 0.784 | C: 162/243 | T: 0:00:36 | es: 35\n",
      "E  35| T | L: 1.55 | S: 0.527 | C: 61/973 | V | L: 0.969 | S: 0.808 | C: 172/243 | T: 0:00:36 | es: 34\n",
      "E  36| T | L: 1.51 | S: 0.52 | C: 61/973 | V | L: 0.931 | S: 0.808 | C: 171/243 | T: 0:00:36 | es: 34\n",
      "E  37| T | L: 1.53 | S: 0.53 | C: 61/973 | V | L: 0.976 | S: 0.806 | C: 169/243 | T: 0:00:37 | es: 33\n",
      "E  38| T | L: 1.51 | S: 0.529 | C: 61/973 | V | L: 1.02 | S: 0.802 | C: 168/243 | T: 0:00:36 | es: 32\n",
      "E  39| T | L: 1.54 | S: 0.519 | C: 61/973 | V | L: 1.02 | S: 0.776 | C: 163/243 | T: 0:00:36 | es: 31\n",
      "E  40| T | L: 1.51 | S: 0.543 | C: 61/973 | V | L: 1.06 | S: 0.806 | C: 173/243 | T: 0:00:36 | es: 30\n",
      "E  41| T | L: 1.53 | S: 0.526 | C: 61/973 | V | L: 1.03 | S: 0.835 | C: 180/243 | T: 0:00:36 | es: 29\n",
      "E  42| T | L: 1.54 | S: 0.529 | C: 61/973 | V | L: 1.03 | S: 0.791 | C: 163/243 | T: 0:00:36 | es: 29\n",
      "E  43| T | L: 1.53 | S: 0.531 | C: 61/973 | V | L: 0.966 | S: 0.803 | C: 169/243 | T: 0:00:36 | es: 28\n",
      "E  44| T | L: 1.52 | S: 0.532 | C: 61/973 | V | L: 1.0 | S: 0.796 | C: 169/243 | T: 0:00:36 | es: 27\n",
      "E  45| T | L: 1.47 | S: 0.551 | C: 61/973 | V | L: 0.922 | S: 0.798 | C: 171/243 | T: 0:00:36 | es: 26\n",
      "E  46| T | L: 1.5 | S: 0.536 | C: 61/973 | V | L: 1.01 | S: 0.806 | C: 171/243 | T: 0:00:36 | es: 25\n",
      "E  47| T | L: 1.47 | S: 0.554 | C: 61/973 | V | L: 0.902 | S: 0.84 | C: 184/243 | T: 0:00:36 | es: 24\n",
      "E  48| T | L: 1.5 | S: 0.551 | C: 61/973 | V | L: 1.06 | S: 0.795 | C: 171/243 | T: 0:00:36 | es: 24\n",
      "E  49| T | L: 1.51 | S: 0.539 | C: 61/973 | V | L: 1.05 | S: 0.793 | C: 165/243 | T: 0:00:36 | es: 23\n",
      "E  50| T | L: 1.48 | S: 0.539 | C: 61/973 | V | L: 0.936 | S: 0.796 | C: 167/243 | T: 0:00:36 | es: 22\n",
      "E  51| T | L: 1.48 | S: 0.556 | C: 61/973 | V | L: 0.894 | S: 0.824 | C: 179/243 | T: 0:00:36 | es: 21\n",
      "E  52| T | L: 1.52 | S: 0.551 | C: 61/973 | V | L: 0.918 | S: 0.809 | C: 169/243 | T: 0:00:36 | es: 20\n",
      "E  53| T | L: 1.54 | S: 0.537 | C: 61/973 | V | L: 0.958 | S: 0.832 | C: 180/243 | T: 0:00:36 | es: 19\n",
      "E  54| T | L: 1.52 | S: 0.554 | C: 61/973 | V | L: 1.02 | S: 0.794 | C: 168/243 | T: 0:00:36 | es: 18\n",
      "E  55| T | L: 1.44 | S: 0.557 | C: 61/973 | V | L: 0.999 | S: 0.783 | C: 166/243 | T: 0:00:36 | es: 17\n",
      "E  56| T | L: 1.49 | S: 0.547 | C: 61/973 | V | L: 0.953 | S: 0.848 | C: 186/243 | T: 0:00:36 | es: 16\n",
      "E  57| T | L: 1.5 | S: 0.548 | C: 61/973 | V | L: 0.934 | S: 0.817 | C: 174/243 | T: 0:00:36 | es: 16\n",
      "E  58| T | L: 1.49 | S: 0.545 | C: 61/973 | V | L: 0.851 | S: 0.838 | C: 185/243 | T: 0:00:36 | es: 15\n",
      "E  59| T | L: 1.48 | S: 0.565 | C: 61/973 | V | L: 0.896 | S: 0.838 | C: 182/243 | T: 0:00:36 | es: 14\n",
      "E  60| T | L: 1.49 | S: 0.557 | C: 61/973 | V | L: 0.896 | S: 0.835 | C: 181/243 | T: 0:00:36 | es: 13\n",
      "E  61| T | L: 1.46 | S: 0.563 | C: 61/973 | V | L: 0.887 | S: 0.847 | C: 183/243 | T: 0:00:36 | es: 12\n",
      "E  62| T | L: 1.49 | S: 0.55 | C: 61/973 | V | L: 1.01 | S: 0.792 | C: 166/243 | T: 0:00:36 | es: 11\n",
      "E  63| T | L: 1.46 | S: 0.55 | C: 61/973 | V | L: 0.92 | S: 0.823 | C: 178/243 | T: 0:00:36 | es: 10\n",
      "E  64| T | L: 1.48 | S: 0.547 | C: 61/973 | V | L: 0.946 | S: 0.811 | C: 169/243 | T: 0:00:36 | es: 9\n",
      "E  65| T | L: 1.48 | S: 0.541 | C: 61/973 | V | L: 0.89 | S: 0.835 | C: 180/243 | T: 0:00:36 | es: 8\n",
      "E  66| T | L: 1.49 | S: 0.541 | C: 61/973 | V | L: 0.978 | S: 0.813 | C: 177/243 | T: 0:00:36 | es: 7\n",
      "E  67| T | L: 1.44 | S: 0.555 | C: 61/973 | V | L: 0.955 | S: 0.835 | C: 182/243 | T: 0:00:36 | es: 6\n",
      "E  68| T | L: 1.45 | S: 0.561 | C: 61/973 | V | L: 0.961 | S: 0.829 | C: 177/243 | T: 0:00:36 | es: 5\n",
      "E  69| T | L: 1.48 | S: 0.558 | C: 61/973 | V | L: 0.961 | S: 0.816 | C: 175/243 | T: 0:00:36 | es: 4\n",
      "E  70| T | L: 1.51 | S: 0.547 | C: 61/973 | V | L: 0.957 | S: 0.835 | C: 182/243 | T: 0:00:36 | es: 3\n",
      "E  71| T | L: 1.52 | S: 0.556 | C: 61/973 | V | L: 0.905 | S: 0.815 | C: 176/243 | T: 0:00:36 | es: 2\n",
      "E  72| T | L: 1.46 | S: 0.567 | C: 61/973 | V | L: 1.02 | S: 0.79 | C: 165/243 | T: 0:00:36 | es: 1\n",
      "bast score: 0.8481548093259335\n",
      "---------- fold 3 ----------\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "E   1| T | L: 3.8 | S: 0.177 | C: 49/973 | V | L: 2.74 | S: 0.197 | C: 18/243 | T: 0:00:36 | es: 50\n",
      "E   2| T | L: 2.06 | S: 0.204 | C: 56/973 | V | L: 2.97 | S: 0.238 | C: 20/243 | T: 0:00:36 | es: 50\n",
      "E   3| T | L: 1.95 | S: 0.219 | C: 57/973 | V | L: 2.25 | S: 0.274 | C: 28/243 | T: 0:00:36 | es: 50\n",
      "E   4| T | L: 1.9 | S: 0.225 | C: 56/973 | V | L: 1.74 | S: 0.368 | C: 51/243 | T: 0:00:36 | es: 50\n",
      "E   5| T | L: 1.86 | S: 0.249 | C: 59/973 | V | L: 1.63 | S: 0.379 | C: 48/243 | T: 0:00:36 | es: 50\n",
      "E   6| T | L: 1.82 | S: 0.277 | C: 60/973 | V | L: 1.64 | S: 0.424 | C: 70/243 | T: 0:00:36 | es: 50\n",
      "E   7| T | L: 1.81 | S: 0.276 | C: 59/973 | V | L: 1.48 | S: 0.468 | C: 73/243 | T: 0:00:36 | es: 50\n",
      "E   8| T | L: 1.78 | S: 0.297 | C: 60/973 | V | L: 1.48 | S: 0.508 | C: 88/243 | T: 0:00:36 | es: 50\n",
      "E   9| T | L: 1.76 | S: 0.313 | C: 61/973 | V | L: 1.39 | S: 0.529 | C: 89/243 | T: 0:00:36 | es: 50\n",
      "E  10| T | L: 1.73 | S: 0.344 | C: 61/973 | V | L: 1.46 | S: 0.45 | C: 68/243 | T: 0:00:36 | es: 50\n",
      "E  11| T | L: 1.75 | S: 0.357 | C: 61/973 | V | L: 1.35 | S: 0.565 | C: 99/243 | T: 0:00:36 | es: 49\n",
      "E  12| T | L: 1.72 | S: 0.368 | C: 61/973 | V | L: 1.28 | S: 0.649 | C: 128/243 | T: 0:00:36 | es: 49\n",
      "E  13| T | L: 1.7 | S: 0.39 | C: 61/973 | V | L: 1.37 | S: 0.562 | C: 100/243 | T: 0:00:36 | es: 49\n",
      "E  14| T | L: 1.71 | S: 0.382 | C: 61/973 | V | L: 1.26 | S: 0.628 | C: 123/243 | T: 0:00:36 | es: 48\n",
      "E  15| T | L: 1.69 | S: 0.405 | C: 61/973 | V | L: 1.21 | S: 0.67 | C: 123/243 | T: 0:00:36 | es: 47\n",
      "E  16| T | L: 1.66 | S: 0.428 | C: 61/973 | V | L: 1.24 | S: 0.695 | C: 136/243 | T: 0:00:36 | es: 47\n",
      "E  17| T | L: 1.64 | S: 0.437 | C: 61/973 | V | L: 1.15 | S: 0.699 | C: 139/243 | T: 0:00:35 | es: 47\n",
      "E  18| T | L: 1.63 | S: 0.441 | C: 61/973 | V | L: 1.12 | S: 0.71 | C: 142/243 | T: 0:00:36 | es: 47\n",
      "E  19| T | L: 1.62 | S: 0.463 | C: 61/973 | V | L: 1.1 | S: 0.751 | C: 159/243 | T: 0:00:36 | es: 47\n",
      "E  20| T | L: 1.62 | S: 0.462 | C: 61/973 | V | L: 1.11 | S: 0.741 | C: 155/243 | T: 0:00:36 | es: 47\n",
      "E  21| T | L: 1.63 | S: 0.468 | C: 61/973 | V | L: 1.13 | S: 0.754 | C: 154/243 | T: 0:00:36 | es: 46\n",
      "E  22| T | L: 1.6 | S: 0.478 | C: 61/973 | V | L: 1.12 | S: 0.759 | C: 157/243 | T: 0:00:36 | es: 46\n",
      "E  23| T | L: 1.57 | S: 0.497 | C: 61/973 | V | L: 0.951 | S: 0.798 | C: 165/243 | T: 0:00:36 | es: 46\n",
      "E  24| T | L: 1.55 | S: 0.49 | C: 61/973 | V | L: 0.985 | S: 0.772 | C: 164/243 | T: 0:00:36 | es: 46\n",
      "E  25| T | L: 1.6 | S: 0.488 | C: 61/973 | V | L: 1.07 | S: 0.783 | C: 167/243 | T: 0:00:36 | es: 45\n",
      "E  26| T | L: 1.55 | S: 0.512 | C: 61/973 | V | L: 1.05 | S: 0.781 | C: 159/243 | T: 0:00:36 | es: 44\n",
      "E  27| T | L: 1.54 | S: 0.514 | C: 61/973 | V | L: 1.06 | S: 0.78 | C: 163/243 | T: 0:00:36 | es: 43\n",
      "E  28| T | L: 1.56 | S: 0.511 | C: 61/973 | V | L: 1.11 | S: 0.737 | C: 155/243 | T: 0:00:36 | es: 42\n",
      "E  29| T | L: 1.51 | S: 0.52 | C: 61/973 | V | L: 0.945 | S: 0.801 | C: 163/243 | T: 0:00:36 | es: 41\n",
      "E  30| T | L: 1.54 | S: 0.522 | C: 61/973 | V | L: 0.995 | S: 0.804 | C: 172/243 | T: 0:00:36 | es: 41\n",
      "E  31| T | L: 1.55 | S: 0.509 | C: 61/973 | V | L: 0.949 | S: 0.822 | C: 173/243 | T: 0:00:36 | es: 41\n",
      "E  32| T | L: 1.53 | S: 0.529 | C: 61/973 | V | L: 0.96 | S: 0.802 | C: 167/243 | T: 0:00:36 | es: 41\n",
      "E  33| T | L: 1.53 | S: 0.524 | C: 61/973 | V | L: 1.01 | S: 0.756 | C: 159/243 | T: 0:00:36 | es: 40\n",
      "E  34| T | L: 1.5 | S: 0.541 | C: 61/973 | V | L: 1.0 | S: 0.787 | C: 165/243 | T: 0:00:36 | es: 39\n",
      "E  35| T | L: 1.56 | S: 0.509 | C: 61/973 | V | L: 0.962 | S: 0.832 | C: 181/243 | T: 0:00:36 | es: 38\n",
      "E  36| T | L: 1.46 | S: 0.542 | C: 61/973 | V | L: 0.89 | S: 0.844 | C: 185/243 | T: 0:00:36 | es: 38\n",
      "E  37| T | L: 1.5 | S: 0.546 | C: 61/973 | V | L: 0.93 | S: 0.808 | C: 174/243 | T: 0:00:36 | es: 38\n",
      "E  38| T | L: 1.5 | S: 0.54 | C: 61/973 | V | L: 0.923 | S: 0.812 | C: 174/243 | T: 0:00:36 | es: 37\n",
      "E  39| T | L: 1.51 | S: 0.546 | C: 61/973 | V | L: 0.993 | S: 0.814 | C: 169/243 | T: 0:00:36 | es: 36\n",
      "E  40| T | L: 1.52 | S: 0.538 | C: 61/973 | V | L: 0.93 | S: 0.84 | C: 184/243 | T: 0:00:36 | es: 35\n",
      "E  41| T | L: 1.5 | S: 0.535 | C: 61/973 | V | L: 0.936 | S: 0.831 | C: 179/243 | T: 0:00:36 | es: 34\n",
      "E  42| T | L: 1.54 | S: 0.533 | C: 61/973 | V | L: 1.04 | S: 0.818 | C: 173/243 | T: 0:00:36 | es: 33\n",
      "E  43| T | L: 1.5 | S: 0.545 | C: 61/973 | V | L: 0.811 | S: 0.838 | C: 179/243 | T: 0:00:36 | es: 32\n",
      "E  44| T | L: 1.51 | S: 0.548 | C: 61/973 | V | L: 1.0 | S: 0.794 | C: 168/243 | T: 0:00:36 | es: 31\n",
      "E  45| T | L: 1.49 | S: 0.541 | C: 61/973 | V | L: 0.89 | S: 0.856 | C: 186/243 | T: 0:00:36 | es: 30\n",
      "E  46| T | L: 1.47 | S: 0.552 | C: 61/973 | V | L: 0.833 | S: 0.845 | C: 184/243 | T: 0:00:36 | es: 30\n",
      "E  47| T | L: 1.46 | S: 0.561 | C: 61/973 | V | L: 0.912 | S: 0.811 | C: 168/243 | T: 0:00:36 | es: 29\n",
      "E  48| T | L: 1.49 | S: 0.546 | C: 61/973 | V | L: 0.895 | S: 0.843 | C: 180/243 | T: 0:00:36 | es: 28\n",
      "E  49| T | L: 1.46 | S: 0.555 | C: 61/973 | V | L: 0.997 | S: 0.803 | C: 172/243 | T: 0:00:36 | es: 27\n",
      "E  50| T | L: 1.48 | S: 0.557 | C: 61/973 | V | L: 0.872 | S: 0.854 | C: 187/243 | T: 0:00:36 | es: 26\n",
      "E  51| T | L: 1.5 | S: 0.55 | C: 61/973 | V | L: 0.886 | S: 0.83 | C: 181/243 | T: 0:00:36 | es: 25\n",
      "E  52| T | L: 1.5 | S: 0.557 | C: 61/973 | V | L: 0.876 | S: 0.831 | C: 185/243 | T: 0:00:36 | es: 24\n",
      "E  53| T | L: 1.49 | S: 0.553 | C: 61/973 | V | L: 0.93 | S: 0.832 | C: 176/243 | T: 0:00:36 | es: 23\n",
      "E  54| T | L: 1.5 | S: 0.564 | C: 61/973 | V | L: 0.896 | S: 0.865 | C: 188/243 | T: 0:00:36 | es: 22\n",
      "E  55| T | L: 1.44 | S: 0.554 | C: 61/973 | V | L: 0.886 | S: 0.849 | C: 183/243 | T: 0:00:36 | es: 22\n",
      "E  56| T | L: 1.48 | S: 0.553 | C: 61/973 | V | L: 1.04 | S: 0.795 | C: 170/243 | T: 0:00:36 | es: 21\n",
      "E  57| T | L: 1.45 | S: 0.563 | C: 61/973 | V | L: 0.894 | S: 0.847 | C: 181/243 | T: 0:00:36 | es: 20\n",
      "E  58| T | L: 1.48 | S: 0.548 | C: 61/973 | V | L: 0.859 | S: 0.838 | C: 184/243 | T: 0:00:36 | es: 19\n",
      "E  59| T | L: 1.46 | S: 0.564 | C: 61/973 | V | L: 0.839 | S: 0.834 | C: 181/243 | T: 0:00:36 | es: 18\n",
      "E  60| T | L: 1.47 | S: 0.562 | C: 61/973 | V | L: 0.918 | S: 0.835 | C: 177/243 | T: 0:00:36 | es: 17\n",
      "E  61| T | L: 1.48 | S: 0.554 | C: 61/973 | V | L: 0.917 | S: 0.81 | C: 176/243 | T: 0:00:36 | es: 16\n",
      "E  62| T | L: 1.48 | S: 0.562 | C: 61/973 | V | L: 0.918 | S: 0.833 | C: 187/243 | T: 0:00:36 | es: 15\n",
      "E  63| T | L: 1.46 | S: 0.555 | C: 61/973 | V | L: 0.883 | S: 0.827 | C: 177/243 | T: 0:00:36 | es: 14\n",
      "E  64| T | L: 1.48 | S: 0.56 | C: 61/973 | V | L: 0.991 | S: 0.798 | C: 167/243 | T: 0:00:36 | es: 13\n",
      "E  65| T | L: 1.43 | S: 0.549 | C: 61/973 | V | L: 0.931 | S: 0.805 | C: 176/243 | T: 0:00:36 | es: 12\n",
      "E  66| T | L: 1.46 | S: 0.557 | C: 61/973 | V | L: 0.887 | S: 0.862 | C: 188/243 | T: 0:00:36 | es: 11\n",
      "E  67| T | L: 1.45 | S: 0.562 | C: 61/973 | V | L: 0.935 | S: 0.812 | C: 171/243 | T: 0:00:36 | es: 10\n",
      "E  68| T | L: 1.44 | S: 0.563 | C: 61/973 | V | L: 0.851 | S: 0.845 | C: 181/243 | T: 0:00:36 | es: 9\n",
      "E  69| T | L: 1.47 | S: 0.572 | C: 61/973 | V | L: 0.842 | S: 0.863 | C: 190/243 | T: 0:00:36 | es: 8\n",
      "E  70| T | L: 1.51 | S: 0.56 | C: 61/973 | V | L: 0.995 | S: 0.83 | C: 175/243 | T: 0:00:36 | es: 7\n",
      "E  71| T | L: 1.51 | S: 0.563 | C: 61/973 | V | L: 0.929 | S: 0.825 | C: 181/243 | T: 0:00:36 | es: 6\n",
      "E  72| T | L: 1.47 | S: 0.555 | C: 61/973 | V | L: 1.15 | S: 0.822 | C: 179/243 | T: 0:00:36 | es: 5\n",
      "E  73| T | L: 1.4 | S: 0.57 | C: 61/973 | V | L: 1.03 | S: 0.776 | C: 172/243 | T: 0:00:36 | es: 4\n",
      "E  74| T | L: 1.47 | S: 0.572 | C: 61/973 | V | L: 0.882 | S: 0.852 | C: 184/243 | T: 0:00:36 | es: 3\n",
      "bast score: 0.8645585402846336\n",
      "---------- fold 4 ----------\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "E   1| T | L: 3.9 | S: 0.188 | C: 48/973 | V | L: 2.42 | S: 0.178 | C: 15/243 | T: 0:00:35 | es: 50\n",
      "E   2| T | L: 2.08 | S: 0.21 | C: 51/973 | V | L: 2.44 | S: 0.238 | C: 28/243 | T: 0:00:36 | es: 50\n",
      "E   3| T | L: 1.95 | S: 0.224 | C: 57/973 | V | L: 2.42 | S: 0.254 | C: 28/243 | T: 0:00:36 | es: 50\n",
      "E   4| T | L: 1.89 | S: 0.244 | C: 58/973 | V | L: 1.78 | S: 0.332 | C: 37/243 | T: 0:00:36 | es: 50\n",
      "E   5| T | L: 1.87 | S: 0.254 | C: 60/973 | V | L: 1.85 | S: 0.384 | C: 59/243 | T: 0:00:36 | es: 50\n",
      "E   6| T | L: 1.83 | S: 0.275 | C: 58/973 | V | L: 1.72 | S: 0.375 | C: 47/243 | T: 0:00:36 | es: 50\n",
      "E   7| T | L: 1.8 | S: 0.309 | C: 61/973 | V | L: 1.54 | S: 0.48 | C: 79/243 | T: 0:00:36 | es: 49\n",
      "E   8| T | L: 1.75 | S: 0.332 | C: 60/973 | V | L: 1.38 | S: 0.517 | C: 78/243 | T: 0:00:36 | es: 49\n",
      "E   9| T | L: 1.77 | S: 0.319 | C: 61/973 | V | L: 1.44 | S: 0.481 | C: 76/243 | T: 0:00:36 | es: 49\n",
      "E  10| T | L: 1.74 | S: 0.346 | C: 61/973 | V | L: 1.37 | S: 0.529 | C: 92/243 | T: 0:00:36 | es: 48\n",
      "E  11| T | L: 1.73 | S: 0.374 | C: 61/973 | V | L: 1.28 | S: 0.581 | C: 105/243 | T: 0:00:36 | es: 48\n",
      "E  12| T | L: 1.71 | S: 0.399 | C: 61/973 | V | L: 1.23 | S: 0.649 | C: 126/243 | T: 0:00:36 | es: 48\n",
      "E  13| T | L: 1.69 | S: 0.397 | C: 61/973 | V | L: 1.27 | S: 0.632 | C: 117/243 | T: 0:00:36 | es: 48\n",
      "E  14| T | L: 1.68 | S: 0.413 | C: 61/973 | V | L: 1.21 | S: 0.669 | C: 127/243 | T: 0:00:36 | es: 47\n",
      "E  15| T | L: 1.68 | S: 0.416 | C: 61/973 | V | L: 1.22 | S: 0.645 | C: 118/243 | T: 0:00:36 | es: 47\n",
      "E  16| T | L: 1.65 | S: 0.443 | C: 61/973 | V | L: 1.24 | S: 0.646 | C: 124/243 | T: 0:00:36 | es: 46\n",
      "E  17| T | L: 1.65 | S: 0.446 | C: 61/973 | V | L: 1.17 | S: 0.722 | C: 144/243 | T: 0:00:36 | es: 45\n",
      "E  18| T | L: 1.62 | S: 0.453 | C: 61/973 | V | L: 1.18 | S: 0.712 | C: 145/243 | T: 0:00:36 | es: 45\n",
      "E  19| T | L: 1.63 | S: 0.454 | C: 61/973 | V | L: 1.14 | S: 0.75 | C: 149/243 | T: 0:00:36 | es: 44\n",
      "E  20| T | L: 1.61 | S: 0.468 | C: 61/973 | V | L: 1.26 | S: 0.661 | C: 121/243 | T: 0:00:36 | es: 44\n",
      "E  21| T | L: 1.62 | S: 0.484 | C: 61/973 | V | L: 1.14 | S: 0.739 | C: 141/243 | T: 0:00:36 | es: 43\n",
      "E  22| T | L: 1.59 | S: 0.498 | C: 61/973 | V | L: 1.04 | S: 0.758 | C: 150/243 | T: 0:00:36 | es: 42\n",
      "E  23| T | L: 1.58 | S: 0.489 | C: 61/973 | V | L: 1.07 | S: 0.771 | C: 154/243 | T: 0:00:36 | es: 42\n",
      "E  24| T | L: 1.53 | S: 0.512 | C: 61/973 | V | L: 1.09 | S: 0.785 | C: 164/243 | T: 0:00:36 | es: 42\n",
      "E  25| T | L: 1.57 | S: 0.504 | C: 61/973 | V | L: 1.06 | S: 0.732 | C: 142/243 | T: 0:00:36 | es: 42\n",
      "E  26| T | L: 1.54 | S: 0.516 | C: 61/973 | V | L: 0.996 | S: 0.791 | C: 161/243 | T: 0:00:36 | es: 41\n",
      "E  27| T | L: 1.54 | S: 0.518 | C: 61/973 | V | L: 1.05 | S: 0.79 | C: 160/243 | T: 0:00:36 | es: 41\n",
      "E  28| T | L: 1.57 | S: 0.515 | C: 61/973 | V | L: 1.14 | S: 0.754 | C: 145/243 | T: 0:00:36 | es: 40\n",
      "E  29| T | L: 1.52 | S: 0.516 | C: 61/973 | V | L: 0.947 | S: 0.845 | C: 180/243 | T: 0:00:36 | es: 39\n",
      "E  30| T | L: 1.55 | S: 0.517 | C: 61/973 | V | L: 1.16 | S: 0.793 | C: 163/243 | T: 0:00:36 | es: 39\n",
      "E  31| T | L: 1.55 | S: 0.521 | C: 61/973 | V | L: 0.94 | S: 0.828 | C: 174/243 | T: 0:00:36 | es: 38\n",
      "E  32| T | L: 1.48 | S: 0.543 | C: 61/973 | V | L: 1.03 | S: 0.793 | C: 165/243 | T: 0:00:36 | es: 37\n",
      "E  33| T | L: 1.55 | S: 0.514 | C: 61/973 | V | L: 0.945 | S: 0.798 | C: 162/243 | T: 0:00:37 | es: 36\n",
      "E  34| T | L: 1.52 | S: 0.528 | C: 61/973 | V | L: 0.873 | S: 0.823 | C: 175/243 | T: 0:00:36 | es: 35\n",
      "E  35| T | L: 1.54 | S: 0.524 | C: 61/973 | V | L: 1.01 | S: 0.797 | C: 164/243 | T: 0:00:36 | es: 34\n",
      "E  36| T | L: 1.5 | S: 0.52 | C: 61/973 | V | L: 0.956 | S: 0.807 | C: 168/243 | T: 0:00:36 | es: 33\n",
      "E  37| T | L: 1.52 | S: 0.537 | C: 61/973 | V | L: 0.956 | S: 0.784 | C: 159/243 | T: 0:00:36 | es: 32\n",
      "E  38| T | L: 1.52 | S: 0.54 | C: 61/973 | V | L: 0.929 | S: 0.828 | C: 172/243 | T: 0:00:36 | es: 31\n",
      "E  39| T | L: 1.51 | S: 0.539 | C: 61/973 | V | L: 0.916 | S: 0.829 | C: 178/243 | T: 0:00:36 | es: 30\n",
      "E  40| T | L: 1.54 | S: 0.538 | C: 61/973 | V | L: 0.91 | S: 0.835 | C: 177/243 | T: 0:00:36 | es: 29\n",
      "E  41| T | L: 1.49 | S: 0.541 | C: 61/973 | V | L: 0.984 | S: 0.792 | C: 161/243 | T: 0:00:36 | es: 28\n",
      "E  42| T | L: 1.53 | S: 0.527 | C: 61/973 | V | L: 1.03 | S: 0.794 | C: 162/243 | T: 0:00:36 | es: 27\n",
      "E  43| T | L: 1.5 | S: 0.547 | C: 61/973 | V | L: 0.926 | S: 0.83 | C: 175/243 | T: 0:00:36 | es: 26\n",
      "E  44| T | L: 1.51 | S: 0.542 | C: 61/973 | V | L: 0.885 | S: 0.85 | C: 183/243 | T: 0:00:36 | es: 25\n",
      "E  45| T | L: 1.48 | S: 0.544 | C: 61/973 | V | L: 0.958 | S: 0.819 | C: 172/243 | T: 0:00:36 | es: 25\n",
      "E  46| T | L: 1.48 | S: 0.55 | C: 61/973 | V | L: 0.882 | S: 0.847 | C: 180/243 | T: 0:00:36 | es: 24\n",
      "E  47| T | L: 1.5 | S: 0.53 | C: 61/973 | V | L: 0.856 | S: 0.852 | C: 184/243 | T: 0:00:36 | es: 23\n",
      "E  48| T | L: 1.51 | S: 0.55 | C: 61/973 | V | L: 1.01 | S: 0.812 | C: 164/243 | T: 0:00:36 | es: 23\n",
      "E  49| T | L: 1.48 | S: 0.545 | C: 61/973 | V | L: 0.948 | S: 0.814 | C: 176/243 | T: 0:00:36 | es: 22\n",
      "E  50| T | L: 1.48 | S: 0.539 | C: 61/973 | V | L: 0.848 | S: 0.827 | C: 175/243 | T: 0:00:36 | es: 21\n",
      "E  51| T | L: 1.49 | S: 0.557 | C: 61/973 | V | L: 0.973 | S: 0.837 | C: 180/243 | T: 0:00:36 | es: 20\n",
      "E  52| T | L: 1.51 | S: 0.547 | C: 61/973 | V | L: 0.979 | S: 0.779 | C: 161/243 | T: 0:00:36 | es: 19\n",
      "E  53| T | L: 1.51 | S: 0.543 | C: 61/973 | V | L: 0.914 | S: 0.849 | C: 185/243 | T: 0:00:36 | es: 18\n",
      "E  54| T | L: 1.53 | S: 0.544 | C: 61/973 | V | L: 0.976 | S: 0.811 | C: 168/243 | T: 0:00:36 | es: 17\n",
      "E  55| T | L: 1.44 | S: 0.559 | C: 61/973 | V | L: 0.974 | S: 0.805 | C: 170/243 | T: 0:00:36 | es: 16\n",
      "E  56| T | L: 1.48 | S: 0.553 | C: 61/973 | V | L: 0.994 | S: 0.823 | C: 173/243 | T: 0:00:36 | es: 15\n",
      "E  57| T | L: 1.48 | S: 0.559 | C: 61/973 | V | L: 0.909 | S: 0.811 | C: 165/243 | T: 0:00:36 | es: 14\n",
      "E  58| T | L: 1.5 | S: 0.538 | C: 61/973 | V | L: 0.881 | S: 0.839 | C: 178/243 | T: 0:00:36 | es: 13\n",
      "E  59| T | L: 1.45 | S: 0.561 | C: 61/973 | V | L: 0.821 | S: 0.846 | C: 180/243 | T: 0:00:36 | es: 12\n",
      "E  60| T | L: 1.46 | S: 0.552 | C: 61/973 | V | L: 0.873 | S: 0.849 | C: 183/243 | T: 0:00:37 | es: 11\n",
      "E  61| T | L: 1.48 | S: 0.552 | C: 61/973 | V | L: 0.899 | S: 0.835 | C: 179/243 | T: 0:00:36 | es: 10\n",
      "E  62| T | L: 1.5 | S: 0.55 | C: 61/973 | V | L: 0.991 | S: 0.827 | C: 175/243 | T: 0:00:36 | es: 9\n",
      "E  63| T | L: 1.45 | S: 0.549 | C: 61/973 | V | L: 0.805 | S: 0.848 | C: 180/243 | T: 0:00:36 | es: 8\n",
      "E  64| T | L: 1.49 | S: 0.542 | C: 61/973 | V | L: 0.845 | S: 0.811 | C: 166/243 | T: 0:00:36 | es: 7\n",
      "E  65| T | L: 1.45 | S: 0.541 | C: 61/973 | V | L: 0.932 | S: 0.817 | C: 169/243 | T: 0:00:36 | es: 6\n",
      "E  66| T | L: 1.49 | S: 0.538 | C: 61/973 | V | L: 0.85 | S: 0.862 | C: 187/243 | T: 0:00:36 | es: 5\n",
      "E  67| T | L: 1.42 | S: 0.56 | C: 61/973 | V | L: 0.942 | S: 0.802 | C: 167/243 | T: 0:00:36 | es: 5\n",
      "E  68| T | L: 1.45 | S: 0.554 | C: 61/973 | V | L: 0.829 | S: 0.834 | C: 177/243 | T: 0:00:36 | es: 4\n",
      "E  69| T | L: 1.47 | S: 0.563 | C: 61/973 | V | L: 0.88 | S: 0.828 | C: 179/243 | T: 0:00:36 | es: 3\n",
      "E  70| T | L: 1.51 | S: 0.548 | C: 61/973 | V | L: 0.976 | S: 0.845 | C: 181/243 | T: 0:00:36 | es: 2\n",
      "E  71| T | L: 1.5 | S: 0.56 | C: 61/973 | V | L: 0.876 | S: 0.854 | C: 185/243 | T: 0:00:36 | es: 1\n",
      "bast score: 0.8622051365673542\n"
     ]
    }
   ],
   "source": [
    "tta = np.zeros((len(submission), 24))\n",
    "cv_score = 0\n",
    "lrs = np.arange(5e-4,1.0e-3,5e-5).tolist()\n",
    "\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(train_tp, train_tp.species_id)):\n",
    "    print(f'---------- fold {fold_id} ----------')\n",
    "    \n",
    "    model = EfficientNetSED(**model_params).to(device)\n",
    "    optim = Adam(model.parameters(), **optim_params)\n",
    "    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optim, **scheduler_params)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=epochs-10)\n",
    "    \n",
    "    pos_weights = torch.ones(24)\n",
    "    pos_weights = pos_weights * 24\n",
    "    criterion = cr.ImprovedFocalLoss(weights=[1, 0.5])\n",
    "    \n",
    "    train_dataset = RfcxDataSet(train_tp.iloc[train_index], **train_params)\n",
    "    val_dataset   = RfcxDataSet(train_tp.iloc[val_index], **val_params)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, **dataloder_params)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=False, **dataloder_params)\n",
    "    \n",
    "    es = 50\n",
    "    bast_score = 0\n",
    "    for epoch in range(1, epochs):\n",
    "        if es <= 0:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        if epoch <= 10:\n",
    "            for g in optim.param_groups:\n",
    "                g['lr'] = lrs[epoch-1]\n",
    "        else:\n",
    "            scheduler.step(val_score)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "        train_corr = 0\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            image = data[0].float().to(device)\n",
    "            label = data[1]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            output, mix_info = model(image)\n",
    "            \n",
    "            mix_info['lam'] = mix_info['lam'].cpu()\n",
    "            output = {k:v.cpu() for k,v in output.items()}\n",
    "            \n",
    "            #loss = mixup_socre(criterion, output, label, mix_info)\n",
    "            loss = criterion(output, label, mix_info)\n",
    "            pred_labels = output[\"framewise_output\"].max(1)[0]\n",
    "            score = mixup_socre(LWLRAP, pred_labels, label, mix_info)\n",
    "            \n",
    "            #score = LWLRAP(pred_labels, label)\n",
    "                        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            vals, answers = torch.max(pred_labels, 1)\n",
    "            vals, targets = torch.max(label, 1)\n",
    "            vals, targets2 = torch.max(label[mix_info['index']], 1)\n",
    "            \n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "                if answers[i] == targets2[i]:\n",
    "                    corrects = corrects + 1\n",
    "                \n",
    "                corrects = 1 if corrects > 0 else 0\n",
    "                    \n",
    "            \n",
    "            train_corr += corrects\n",
    "            train_loss += loss.item()\n",
    "            train_score += score\n",
    "            \n",
    "        train_loss  /= len(train_dataloader)\n",
    "        train_score /= len(train_dataloader)\n",
    "        \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        val_corr = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_dataloader:\n",
    "                image = val_data[0].float().to(device)\n",
    "                label = val_data[1]\n",
    "\n",
    "                output, mix_info = model(image)\n",
    "                output = {k:v.cpu() for k,v in output.items()}\n",
    "                \n",
    "                pred_labels = output[\"framewise_output\"].max(1)[0]\n",
    "                vals, answers = torch.max(pred_labels, 1)\n",
    "                vals, targets = torch.max(label, 1)\n",
    "                \n",
    "                corrects = 0\n",
    "                for i in range(0, len(answers)):\n",
    "                    if answers[i] == targets[i]:\n",
    "                        corrects = corrects + 1\n",
    "\n",
    "                val_corr += corrects\n",
    "                val_loss += criterion(output, label)\n",
    "                val_score += LWLRAP(pred_labels, label)\n",
    "                \n",
    "        val_loss  /= len(val_dataloader)\n",
    "        val_score /= len(val_dataloader)\n",
    "        \n",
    "        duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
    "        print(f'E {epoch:3}| T | L: {train_loss:.3} | S: {train_score:.3} | C: {train_corr}/{len(train_dataset)} | V | L: {val_loss:.3} | S: {val_score:.3} | C: {val_corr}/{len(val_dataset)} | T: {duration} | es: {es}')\n",
    "\n",
    "        if bast_score < val_score:\n",
    "            bast_score = val_score\n",
    "            bast_path = save(fold_id, model, optim, criterion)\n",
    "        else:\n",
    "            es -= 1\n",
    "        \n",
    "        if es <= 0:\n",
    "            break\n",
    "            \n",
    "        #scheduler.step(val_score)\n",
    "    \n",
    "    print(f\"bast score: {bast_score}\")\n",
    "    del model, train_dataset, val_dataset, train_dataloader, val_dataloader, optim\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel(y, img_size=256): \n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        y,\n",
    "        sr=SR,\n",
    "        fmin=0,\n",
    "        fmax=15000,\n",
    "        n_mels=128\n",
    "    )\n",
    "\n",
    "    pcen = librosa.pcen(melspec, sr=SR, **pcen_parameters)\n",
    "    clean_mel = librosa.power_to_db(melspec ** 1.5)\n",
    "    melspec = librosa.power_to_db(melspec)\n",
    "\n",
    "    norm_melspec = normalize_melspec(melspec)\n",
    "    norm_pcen = normalize_melspec(pcen)\n",
    "    norm_clean_mel = normalize_melspec(clean_mel)\n",
    "\n",
    "    image = np.stack([norm_melspec, norm_pcen, norm_clean_mel], axis=-1)\n",
    "    \n",
    "    height, width, _ = image.shape\n",
    "    image = cv2.resize(image, (img_size * 2, img_size))\n",
    "    image = np.moveaxis(image, 2, 0)\n",
    "    image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "    return image\n",
    "\n",
    "def prediction_for_clip(audio_id: str,\n",
    "                        clip: np.ndarray, \n",
    "                        model: EfficientNetSED,\n",
    "                        threshold=0.5):\n",
    "    PERIOD = 10\n",
    "    audios = []\n",
    "    y = clip.astype(np.float32)\n",
    "    len_y = len(y)\n",
    "    start = 0\n",
    "    end = PERIOD * SR\n",
    "    while True:\n",
    "        y_batch = y[start:end].astype(np.float32)\n",
    "\n",
    "        start = end - (5 * SR)\n",
    "        end += 5 * SR\n",
    "        \n",
    "        #mel = create_mel(y_batch)\n",
    "        audios.append(y_batch)\n",
    "        \n",
    "        if len_y < end:\n",
    "            break\n",
    "            \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    array = np.asarray(audios)\n",
    "    image = torch.from_numpy(array).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    estimated_event_list = []\n",
    "    global_time = 0.0\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction, _ = model(image)\n",
    "        frame_pred = torch.sum(\n",
    "            torch.sigmoid(torch.max(prediction[\"framewise_output\"], 1)[0]), 0\n",
    "        ).detach().cpu().numpy()\n",
    "        framewise_outputs = torch.max(prediction[\"framewise_output\"], 0)[0].detach(\n",
    "            ).cpu().numpy()\n",
    "        \n",
    "    thresholded = framewise_outputs >= threshold\n",
    "    \n",
    "    for target_idx in range(thresholded.shape[1]):\n",
    "        if thresholded[:, target_idx].mean() == 0:\n",
    "            pass\n",
    "        else:\n",
    "            detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
    "            head_idx = 0\n",
    "            tail_idx = 0\n",
    "            while True:\n",
    "                if (tail_idx + 1 == len(detected)) or (\n",
    "                        detected[tail_idx + 1] - \n",
    "                        detected[tail_idx] != 1):\n",
    "                    onset = 0.01 * detected[\n",
    "                        head_idx] + global_time\n",
    "                    offset = 0.01 * detected[\n",
    "                        tail_idx] + global_time\n",
    "                    onset_idx = detected[head_idx]\n",
    "                    offset_idx = detected[tail_idx]\n",
    "                    max_confidence = framewise_outputs[\n",
    "                        onset_idx:offset_idx, target_idx].max()\n",
    "                    mean_confidence = framewise_outputs[\n",
    "                        onset_idx:offset_idx, target_idx].mean()\n",
    "                    estimated_event = {\n",
    "                        \"audio_id\": audio_id,\n",
    "                        \"ebird_code\": target_idx,\n",
    "                        \"onset\": onset,\n",
    "                        \"offset\": offset,\n",
    "                        \"max_confidence\": max_confidence,\n",
    "                        \"mean_confidence\": mean_confidence\n",
    "                    }\n",
    "                    estimated_event_list.append(estimated_event)\n",
    "                    head_idx = tail_idx + 1\n",
    "                    tail_idx = tail_idx + 1\n",
    "                    if head_idx >= len(detected):\n",
    "                        break\n",
    "                else:\n",
    "                    tail_idx += 1\n",
    "        global_time += PERIOD\n",
    "        \n",
    "    prediction_df = pd.DataFrame(estimated_event_list)\n",
    "    return prediction_df, frame_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_df: pd.DataFrame,\n",
    "               model: dict,\n",
    "               threshold=0.5):\n",
    "    #model = get_model(model_config, weights_path)\n",
    "    unique_audio_id = test_df.recording_id.unique()\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dfs = []\n",
    "    frame_dict = dict()\n",
    "    for audio_id in tqdm(unique_audio_id):\n",
    "        clip, _ = librosa.load(\n",
    "            f'/home/yuigahama/kaggle/rfcx/data/test/{audio_id}.flac',\n",
    "            sr=SR,\n",
    "            mono=True,\n",
    "            res_type=\"kaiser_fast\"\n",
    "        )\n",
    "        \n",
    "        test_df_for_audio_id = test_df.query(\n",
    "            f\"recording_id == '{audio_id}'\").reset_index(drop=True)\n",
    "        prediction_df, frame_pred = prediction_for_clip(\n",
    "            audio_id,\n",
    "            clip=clip,\n",
    "            model=model,\n",
    "            threshold=threshold\n",
    "        )\n",
    "        frame_dict[audio_id] = frame_pred\n",
    "        prediction_dfs.append(prediction_df)\n",
    "    \n",
    "    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "    return prediction_df, frame_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [02:58<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [02:57<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [02:56<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [02:57<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [02:57<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "frams = [] \n",
    "for i in range(5):\n",
    "\n",
    "    model = EfficientNetSED(**model_params).to(device)\n",
    "    params = torch.load(f'/home/yuigahama/kaggle/rfcx/model/{TEST_NAME}/{TEST_NAME}_{i}.model')\n",
    "    model.load_state_dict(params['model_state_dict'])\n",
    "    \n",
    "    prediction_df, frame_dict = prediction(\n",
    "        test_df=submission,\n",
    "        model=model,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    print(len(prediction_df.audio_id.unique()))\n",
    "    preds.append(prediction_df)\n",
    "    frams.append(pd.DataFrame(frame_dict).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s0</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>...</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>s22</th>\n",
       "      <th>s23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recording_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000316da7</th>\n",
       "      <td>5.536902</td>\n",
       "      <td>5.428552</td>\n",
       "      <td>5.398044</td>\n",
       "      <td>5.666194</td>\n",
       "      <td>5.455990</td>\n",
       "      <td>5.579698</td>\n",
       "      <td>5.493617</td>\n",
       "      <td>5.528298</td>\n",
       "      <td>5.443487</td>\n",
       "      <td>5.436513</td>\n",
       "      <td>...</td>\n",
       "      <td>5.445397</td>\n",
       "      <td>5.501407</td>\n",
       "      <td>5.508829</td>\n",
       "      <td>5.512059</td>\n",
       "      <td>5.598620</td>\n",
       "      <td>5.435609</td>\n",
       "      <td>5.434749</td>\n",
       "      <td>5.431371</td>\n",
       "      <td>5.435540</td>\n",
       "      <td>5.563958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003bc2cb2</th>\n",
       "      <td>5.357302</td>\n",
       "      <td>5.459712</td>\n",
       "      <td>5.364480</td>\n",
       "      <td>5.604771</td>\n",
       "      <td>5.408080</td>\n",
       "      <td>5.477409</td>\n",
       "      <td>5.444747</td>\n",
       "      <td>5.527529</td>\n",
       "      <td>5.391532</td>\n",
       "      <td>5.397219</td>\n",
       "      <td>...</td>\n",
       "      <td>5.412023</td>\n",
       "      <td>5.435459</td>\n",
       "      <td>6.025512</td>\n",
       "      <td>5.513920</td>\n",
       "      <td>5.413760</td>\n",
       "      <td>5.381675</td>\n",
       "      <td>5.397167</td>\n",
       "      <td>5.401389</td>\n",
       "      <td>5.406348</td>\n",
       "      <td>5.530141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0061c037e</th>\n",
       "      <td>5.445302</td>\n",
       "      <td>5.472865</td>\n",
       "      <td>5.455980</td>\n",
       "      <td>5.592596</td>\n",
       "      <td>5.463711</td>\n",
       "      <td>5.553733</td>\n",
       "      <td>5.510018</td>\n",
       "      <td>5.648340</td>\n",
       "      <td>5.446731</td>\n",
       "      <td>5.444982</td>\n",
       "      <td>...</td>\n",
       "      <td>5.495149</td>\n",
       "      <td>5.516629</td>\n",
       "      <td>5.571565</td>\n",
       "      <td>5.572390</td>\n",
       "      <td>5.468091</td>\n",
       "      <td>5.546052</td>\n",
       "      <td>5.485317</td>\n",
       "      <td>5.478624</td>\n",
       "      <td>5.497894</td>\n",
       "      <td>5.665424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010eb14d3</th>\n",
       "      <td>5.971642</td>\n",
       "      <td>5.446492</td>\n",
       "      <td>5.372373</td>\n",
       "      <td>5.485849</td>\n",
       "      <td>5.487978</td>\n",
       "      <td>5.503754</td>\n",
       "      <td>5.454111</td>\n",
       "      <td>5.486998</td>\n",
       "      <td>5.860940</td>\n",
       "      <td>5.429837</td>\n",
       "      <td>...</td>\n",
       "      <td>5.420577</td>\n",
       "      <td>5.491217</td>\n",
       "      <td>5.431955</td>\n",
       "      <td>5.484108</td>\n",
       "      <td>5.632305</td>\n",
       "      <td>5.417309</td>\n",
       "      <td>5.389531</td>\n",
       "      <td>5.512753</td>\n",
       "      <td>5.397875</td>\n",
       "      <td>5.533357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>011318064</th>\n",
       "      <td>5.440906</td>\n",
       "      <td>5.468907</td>\n",
       "      <td>5.410848</td>\n",
       "      <td>5.581291</td>\n",
       "      <td>5.437844</td>\n",
       "      <td>5.511499</td>\n",
       "      <td>5.461808</td>\n",
       "      <td>5.537067</td>\n",
       "      <td>5.443908</td>\n",
       "      <td>5.413940</td>\n",
       "      <td>...</td>\n",
       "      <td>6.074458</td>\n",
       "      <td>5.612804</td>\n",
       "      <td>5.481584</td>\n",
       "      <td>5.536711</td>\n",
       "      <td>5.543149</td>\n",
       "      <td>5.436938</td>\n",
       "      <td>5.450940</td>\n",
       "      <td>5.467243</td>\n",
       "      <td>5.436522</td>\n",
       "      <td>5.532111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff68f3ac3</th>\n",
       "      <td>5.530192</td>\n",
       "      <td>5.399386</td>\n",
       "      <td>5.401857</td>\n",
       "      <td>5.546243</td>\n",
       "      <td>5.458876</td>\n",
       "      <td>6.027565</td>\n",
       "      <td>5.453616</td>\n",
       "      <td>5.557828</td>\n",
       "      <td>5.453488</td>\n",
       "      <td>5.393347</td>\n",
       "      <td>...</td>\n",
       "      <td>5.432716</td>\n",
       "      <td>5.820098</td>\n",
       "      <td>5.434603</td>\n",
       "      <td>5.494606</td>\n",
       "      <td>5.482264</td>\n",
       "      <td>5.421736</td>\n",
       "      <td>5.404167</td>\n",
       "      <td>5.400099</td>\n",
       "      <td>5.394108</td>\n",
       "      <td>5.917825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff973e852</th>\n",
       "      <td>5.420998</td>\n",
       "      <td>5.432414</td>\n",
       "      <td>5.389553</td>\n",
       "      <td>5.551100</td>\n",
       "      <td>5.405830</td>\n",
       "      <td>5.525096</td>\n",
       "      <td>5.494099</td>\n",
       "      <td>5.921758</td>\n",
       "      <td>5.417062</td>\n",
       "      <td>5.584220</td>\n",
       "      <td>...</td>\n",
       "      <td>5.479837</td>\n",
       "      <td>5.614740</td>\n",
       "      <td>5.488991</td>\n",
       "      <td>5.836863</td>\n",
       "      <td>5.448909</td>\n",
       "      <td>5.447275</td>\n",
       "      <td>5.499389</td>\n",
       "      <td>5.401888</td>\n",
       "      <td>5.463620</td>\n",
       "      <td>5.581737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffa5cf6d6</th>\n",
       "      <td>5.409090</td>\n",
       "      <td>5.475642</td>\n",
       "      <td>5.441834</td>\n",
       "      <td>5.638551</td>\n",
       "      <td>5.404203</td>\n",
       "      <td>5.503583</td>\n",
       "      <td>5.474242</td>\n",
       "      <td>5.582300</td>\n",
       "      <td>5.405985</td>\n",
       "      <td>5.469386</td>\n",
       "      <td>...</td>\n",
       "      <td>5.436937</td>\n",
       "      <td>5.824130</td>\n",
       "      <td>5.549417</td>\n",
       "      <td>5.601403</td>\n",
       "      <td>5.450568</td>\n",
       "      <td>5.434418</td>\n",
       "      <td>5.433962</td>\n",
       "      <td>5.383908</td>\n",
       "      <td>5.431250</td>\n",
       "      <td>5.570698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffa88cbb8</th>\n",
       "      <td>5.406117</td>\n",
       "      <td>5.616352</td>\n",
       "      <td>5.426845</td>\n",
       "      <td>5.666128</td>\n",
       "      <td>5.428423</td>\n",
       "      <td>5.520968</td>\n",
       "      <td>5.417374</td>\n",
       "      <td>5.724147</td>\n",
       "      <td>5.400343</td>\n",
       "      <td>5.503032</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419724</td>\n",
       "      <td>5.491212</td>\n",
       "      <td>5.708699</td>\n",
       "      <td>5.486681</td>\n",
       "      <td>5.460002</td>\n",
       "      <td>5.383749</td>\n",
       "      <td>5.452403</td>\n",
       "      <td>5.459299</td>\n",
       "      <td>5.442833</td>\n",
       "      <td>5.563057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffda5d7b3</th>\n",
       "      <td>5.391147</td>\n",
       "      <td>5.434715</td>\n",
       "      <td>5.985314</td>\n",
       "      <td>5.594075</td>\n",
       "      <td>5.408179</td>\n",
       "      <td>5.532643</td>\n",
       "      <td>5.441004</td>\n",
       "      <td>5.498883</td>\n",
       "      <td>5.417299</td>\n",
       "      <td>5.366991</td>\n",
       "      <td>...</td>\n",
       "      <td>5.406820</td>\n",
       "      <td>5.469648</td>\n",
       "      <td>5.476295</td>\n",
       "      <td>5.519035</td>\n",
       "      <td>5.455279</td>\n",
       "      <td>5.499663</td>\n",
       "      <td>5.419748</td>\n",
       "      <td>5.388942</td>\n",
       "      <td>5.508211</td>\n",
       "      <td>5.515653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1992 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    s0        s1        s2        s3        s4        s5  \\\n",
       "recording_id                                                               \n",
       "000316da7     5.536902  5.428552  5.398044  5.666194  5.455990  5.579698   \n",
       "003bc2cb2     5.357302  5.459712  5.364480  5.604771  5.408080  5.477409   \n",
       "0061c037e     5.445302  5.472865  5.455980  5.592596  5.463711  5.553733   \n",
       "010eb14d3     5.971642  5.446492  5.372373  5.485849  5.487978  5.503754   \n",
       "011318064     5.440906  5.468907  5.410848  5.581291  5.437844  5.511499   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "ff68f3ac3     5.530192  5.399386  5.401857  5.546243  5.458876  6.027565   \n",
       "ff973e852     5.420998  5.432414  5.389553  5.551100  5.405830  5.525096   \n",
       "ffa5cf6d6     5.409090  5.475642  5.441834  5.638551  5.404203  5.503583   \n",
       "ffa88cbb8     5.406117  5.616352  5.426845  5.666128  5.428423  5.520968   \n",
       "ffda5d7b3     5.391147  5.434715  5.985314  5.594075  5.408179  5.532643   \n",
       "\n",
       "                    s6        s7        s8        s9  ...       s14       s15  \\\n",
       "recording_id                                          ...                       \n",
       "000316da7     5.493617  5.528298  5.443487  5.436513  ...  5.445397  5.501407   \n",
       "003bc2cb2     5.444747  5.527529  5.391532  5.397219  ...  5.412023  5.435459   \n",
       "0061c037e     5.510018  5.648340  5.446731  5.444982  ...  5.495149  5.516629   \n",
       "010eb14d3     5.454111  5.486998  5.860940  5.429837  ...  5.420577  5.491217   \n",
       "011318064     5.461808  5.537067  5.443908  5.413940  ...  6.074458  5.612804   \n",
       "...                ...       ...       ...       ...  ...       ...       ...   \n",
       "ff68f3ac3     5.453616  5.557828  5.453488  5.393347  ...  5.432716  5.820098   \n",
       "ff973e852     5.494099  5.921758  5.417062  5.584220  ...  5.479837  5.614740   \n",
       "ffa5cf6d6     5.474242  5.582300  5.405985  5.469386  ...  5.436937  5.824130   \n",
       "ffa88cbb8     5.417374  5.724147  5.400343  5.503032  ...  5.419724  5.491212   \n",
       "ffda5d7b3     5.441004  5.498883  5.417299  5.366991  ...  5.406820  5.469648   \n",
       "\n",
       "                   s16       s17       s18       s19       s20       s21  \\\n",
       "recording_id                                                               \n",
       "000316da7     5.508829  5.512059  5.598620  5.435609  5.434749  5.431371   \n",
       "003bc2cb2     6.025512  5.513920  5.413760  5.381675  5.397167  5.401389   \n",
       "0061c037e     5.571565  5.572390  5.468091  5.546052  5.485317  5.478624   \n",
       "010eb14d3     5.431955  5.484108  5.632305  5.417309  5.389531  5.512753   \n",
       "011318064     5.481584  5.536711  5.543149  5.436938  5.450940  5.467243   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "ff68f3ac3     5.434603  5.494606  5.482264  5.421736  5.404167  5.400099   \n",
       "ff973e852     5.488991  5.836863  5.448909  5.447275  5.499389  5.401888   \n",
       "ffa5cf6d6     5.549417  5.601403  5.450568  5.434418  5.433962  5.383908   \n",
       "ffa88cbb8     5.708699  5.486681  5.460002  5.383749  5.452403  5.459299   \n",
       "ffda5d7b3     5.476295  5.519035  5.455279  5.499663  5.419748  5.388942   \n",
       "\n",
       "                   s22       s23  \n",
       "recording_id                      \n",
       "000316da7     5.435540  5.563958  \n",
       "003bc2cb2     5.406348  5.530141  \n",
       "0061c037e     5.497894  5.665424  \n",
       "010eb14d3     5.397875  5.533357  \n",
       "011318064     5.436522  5.532111  \n",
       "...                ...       ...  \n",
       "ff68f3ac3     5.394108  5.917825  \n",
       "ff973e852     5.463620  5.581737  \n",
       "ffa5cf6d6     5.431250  5.570698  \n",
       "ffa88cbb8     5.442833  5.563057  \n",
       "ffda5d7b3     5.508211  5.515653  \n",
       "\n",
       "[1992 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(np.zeros((len(submission), 24)),columns=pred_target, index=submission['recording_id'])\n",
    "for p,j in zip(frams, [0.1, 0.233, 0.1, 0.233, 0.234]):\n",
    "    p.columns = pred_target\n",
    "    sub += (p * j)\n",
    "#sub /= 5\n",
    "sub.reset_index().to_csv('submission.csv', index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(pd.concat(preds).audio_id.unique()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prediction_df = pd.concat(preds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(prediction_df.audio_id.unique())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preds_tmp = []\n",
    "\n",
    "for pdf in preds:\n",
    "    labels = {}\n",
    "    for audio_id, sub_df in pdf.groupby(\"audio_id\"):\n",
    "        events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\"]].values\n",
    "        n_events = len(events)\n",
    "        removed_event = []\n",
    "        # Overlap deletion: this part may not be necessary\n",
    "        # I deleted this part in other model and found there's no difference on the public LB score.\n",
    "        for i in range(n_events):\n",
    "            for j in range(n_events):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if i in removed_event:\n",
    "                    continue\n",
    "                if j in removed_event:\n",
    "                    continue\n",
    "\n",
    "                event_i = events[i]\n",
    "                event_j = events[j]\n",
    "\n",
    "                if (event_i[1] - event_j[2] >= 0) or (event_j[1] - event_i[2] >= 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    later_onset = max(event_i[1], event_j[1])\n",
    "                    sooner_onset = min(event_i[1], event_j[1])\n",
    "                    sooner_offset = min(event_i[2], event_j[2])\n",
    "                    later_offset = max(event_i[2], event_j[2])\n",
    "\n",
    "                    intersection = sooner_offset - later_onset\n",
    "                    union = later_offset - sooner_onset\n",
    "\n",
    "                    iou = intersection / union\n",
    "                    if iou > 0.4:\n",
    "                        if event_i[3] > event_j[3]:\n",
    "                            removed_event.append(j)\n",
    "                        else:\n",
    "                            removed_event.append(i)\n",
    "\n",
    "        for i in range(n_events):\n",
    "            if i in removed_event:\n",
    "                continue\n",
    "            event = events[i][0]\n",
    "            onset = events[i][1]\n",
    "            offset = events[i][2]\n",
    "            max_confidence = events[i][3]\n",
    "            row_id = f\"{audio_id}\"\n",
    "            if labels.get(row_id) is not None:\n",
    "                labels[row_id].add((int(event), float(max_confidence)))\n",
    "            else:\n",
    "                labels[row_id] = set()\n",
    "                labels[row_id].add((int(event), float(max_confidence)))\n",
    "    preds_tmp.append(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_pred_dfs = []\n",
    "for labels in preds_tmp:\n",
    "    for key in labels:\n",
    "        labels[key] = \" \".join([\",\".join([str(j) for j in i]) for i in sorted(list(labels[key]))])\n",
    "    \n",
    "    \n",
    "    row_ids = list(labels.keys())\n",
    "    birds = list(labels.values())\n",
    "    post_processed = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"birds\": birds\n",
    "    })\n",
    "    tmp_pred_dfs.append(post_processed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subs_df = []\n",
    "\n",
    "for tpd in tmp_pred_dfs:\n",
    "    sub = pd.DataFrame(np.zeros((len(submission), 24)),columns=pred_target, index=submission['recording_id'])\n",
    "\n",
    "    for i in range(len(tpd)):\n",
    "        sample = tpd.iloc[i]\n",
    "\n",
    "        for c in sample['birds'].split(' '):\n",
    "            label, p = c.split(',')\n",
    "            sub.loc[sample['row_id'], f\"s{label}\"] = float(p)\n",
    "    subs_df.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([0.1, 0.233, 0.1, 0.233, 0.234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0005,\n",
       "  0.00055,\n",
       "  0.0006000000000000001,\n",
       "  0.0006500000000000001,\n",
       "  0.0007000000000000001,\n",
       "  0.0007500000000000001,\n",
       "  0.0008000000000000001,\n",
       "  0.0008500000000000002,\n",
       "  0.0009000000000000002,\n",
       "  0.0009500000000000002],\n",
       " 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs = np.arange(5e-4,1.0e-3,5e-5).tolist()\n",
    "lrs, len(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-78f7dcbb337e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([[1],[2],[3]]).size())\n",
    "a*torch.tensor([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "((subs_df[0] + subs_df[1] + subs_df[2] + subs_df[3] + subs_df[4]) / 5).reset_index().to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
